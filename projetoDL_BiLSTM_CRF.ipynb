{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bergoliveira/disciplinaDL/blob/main/projetoDL_BiLSTM_CRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c051100"
      },
      "source": [
        "# 1. Gerais\n",
        "Adaptado de https://github.com/peluz/lener-br e https://cic.unb.br/~teodecampos/LeNER-Br/\n",
        "\n",
        "# 2. Estrutura de diretórios\n",
        "* ./data/lener_original/train, test, dev\n",
        " - treinamento, teste e desenvolvimento\n",
        " - formato: IOB2003"
      ],
      "id": "9c051100"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFVqLNSBN5fS",
        "outputId": "d4a7641e-5b72-471d-e802-1a9f0f51df06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 16 14:47:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#teste gpu\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "rFVqLNSBN5fS"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6RkbV2fqC3d",
        "outputId": "79889aac-d05b-430e-ab39-bd5a96c5f351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#teste ram\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "C6RkbV2fqC3d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8vb_VzndAf6",
        "outputId": "4723fc79-c259-4b45-a1a1-5559e46bf738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#drive e variáveis globais\n",
        "#utilizar GPU\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "J8vb_VzndAf6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "2b103112",
        "outputId": "9535b9fc-4759-4d44-d006-4e37f0a2633b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "TensorFlow 1.x selected.\n",
            "Python 3.7.12\n",
            "pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Nltk 3.2.5 \n",
            "Numpy 1.21.5 \n",
            "Sklearn 1.0.2 \n",
            "Tensorflow 1.15.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"try:\\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\\nexcept ValueError:\\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\\n\\ntf.config.experimental_connect_to_cluster(tpu)\\ntf.tpu.experimental.initialize_tpu_system(tpu)\\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "#https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=NeWVBhf1VxlH\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf\n",
        "\n",
        "#DIR = BASE DE DADOS\n",
        "DIR = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_LenerBr/data/avaliacao_hyper/lener/'\n",
        "\n",
        "#versões\n",
        "!python --version\n",
        "!pip --version\n",
        "print(f'Nltk {nltk.__version__} \\\n",
        "\\nNumpy {np.__version__} \\\n",
        "\\nSklearn {sklearn.__version__} \\\n",
        "\\nTensorflow {tf.__version__}')\n",
        "\n",
        "#teste tpu fuincionando\n",
        "'''try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)'''"
      ],
      "id": "2b103112"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BSk3gu9gzvZ"
      },
      "source": [
        "## 3. Configurações do modelo"
      ],
      "id": "-BSk3gu9gzvZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Dados"
      ],
      "metadata": {
        "id": "bPONohvQCbwZ"
      },
      "id": "bPONohvQCbwZ"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6rTIJHA_APMy"
      },
      "outputs": [],
      "source": [
        "#data_utils.py\n",
        "# shared global variables to be imported from model also\n",
        "UNK = \"$UNK$\"\n",
        "NUM = \"$NUM$\"\n",
        "NONE = \"O\"\n",
        "\n",
        "\n",
        "# special error message\n",
        "class MyIOError(Exception):\n",
        "    def __init__(self, filename):\n",
        "        # custom error message\n",
        "        message = \"\"\"\n",
        "ERROR: Unable to locate file {}.\n",
        "\n",
        "\n",
        "\"\"\".format(filename)\n",
        "        super(MyIOError, self).__init__(message)\n",
        "\n",
        "\n",
        "class CoNLLDataset(object):\n",
        "    #Class that iterates over CoNLL Dataset\n",
        "\n",
        "    def __init__(self, filename, processing_word=None, processing_tag=None,\n",
        "                 max_iter=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            filename: path to the file\n",
        "            processing_words: (optional) function that takes a word as input\n",
        "            processing_tags: (optional) function that takes a tag as input\n",
        "            max_iter: (optional) max number of sentences to yield\n",
        "\n",
        "        \"\"\"\n",
        "        self.filename = filename\n",
        "        self.processing_word = processing_word\n",
        "        self.processing_tag = processing_tag\n",
        "        self.max_iter = max_iter\n",
        "        self.length = None\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        niter = 0\n",
        "        n_line = 0\n",
        "        with open(self.filename) as f:\n",
        "            words, tags = [], []\n",
        "            for line in f:\n",
        "                n_line += 1\n",
        "                line = line.strip()\n",
        "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
        "                    if len(words) != 0:\n",
        "                        niter += 1\n",
        "                        if self.max_iter is not None and niter > self.max_iter:\n",
        "                            break\n",
        "                        yield words, tags\n",
        "                        words, tags = [], []\n",
        "                else:\n",
        "                    ls = line.split(' ')\n",
        "                    word, tag = ls[0],ls[-1]\n",
        "                    if tag == ',':\n",
        "                        print(n_line)\n",
        "                    if self.processing_word is not None:\n",
        "                        word = self.processing_word(word)\n",
        "                    if self.processing_tag is not None:\n",
        "                        tag = self.processing_tag(tag)\n",
        "                    words += [word]\n",
        "                    tags += [tag]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Iterates once over the corpus to set and store length\"\"\"\n",
        "        if self.length is None:\n",
        "            self.length = 0\n",
        "            for _ in self:\n",
        "                self.length += 1\n",
        "\n",
        "        return self.length\n",
        "\n",
        "\n",
        "def get_vocabs(datasets):\n",
        "    \"\"\"Build vocabulary from an iterable of datasets objects\n",
        "\n",
        "    Args:\n",
        "        datasets: a list of dataset objects\n",
        "    Returns:\n",
        "        a set of all the words in the dataset\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Building vocab...\")\n",
        "    vocab_words = set()\n",
        "    vocab_tags = set()\n",
        "    for dataset in datasets:\n",
        "        for words, tags in dataset:\n",
        "            vocab_words.update(words)\n",
        "            vocab_tags.update(tags)\n",
        "    print(\"- done. {} tokens\".format(len(vocab_words)))\n",
        "    return vocab_words, vocab_tags\n",
        "\n",
        "\n",
        "def get_char_vocab(dataset):\n",
        "    \"\"\"Build char vocabulary from an iterable of datasets objects\n",
        "\n",
        "    Args:\n",
        "        dataset: a iterator yielding tuples (sentence, tags)\n",
        "    Returns:\n",
        "        a set of all the characters in the dataset\n",
        "\n",
        "    \"\"\"\n",
        "    vocab_char = set()\n",
        "    for words, _ in dataset:\n",
        "        for word in words:\n",
        "            vocab_char.update(word)\n",
        "\n",
        "    return vocab_char\n",
        "\n",
        "\n",
        "def get_glove_vocab(filename):\n",
        "    \"\"\"Load vocab from file\n",
        "\n",
        "    Args:\n",
        "        filename: path to the glove vectors\n",
        "    Returns:\n",
        "        vocab: set() of strings\n",
        "    \"\"\"\n",
        "    print(\"Building vocab...\")\n",
        "    vocab = set()\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            word = line.strip().split(' ')[0]\n",
        "            vocab.add(word)\n",
        "    print(\"- done. {} tokens\".format(len(vocab)))\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def write_vocab(vocab, filename):\n",
        "    \"\"\"Writes a vocab to a file\n",
        "       Writes one word per line.\n",
        "    \n",
        "    Args:\n",
        "        vocab: iterable that yields word\n",
        "        filename: path to vocab file\n",
        "    Returns:\n",
        "        write a word per line\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Writing vocab...\")\n",
        "    with open(filename, \"w\") as f:\n",
        "        for i, word in enumerate(vocab):\n",
        "            if i != len(vocab) - 1:\n",
        "                f.write(\"{}\\n\".format(word))\n",
        "            else:\n",
        "                f.write(word)\n",
        "    print(\"- done. {} tokens\".format(len(vocab)))\n",
        "\n",
        "\n",
        "def load_vocab(filename):\n",
        "    \"\"\"Loads vocab from a file\n",
        "\n",
        "    Args:\n",
        "        filename: (string) the format of the file must be one word per line.\n",
        "    Returns:\n",
        "        d: dict[word] = index\n",
        "    \"\"\"\n",
        "    try:\n",
        "        d = dict()\n",
        "        with open(filename) as f:\n",
        "            for idx, word in enumerate(f):\n",
        "                word = word.strip()\n",
        "                d[word] = idx\n",
        "\n",
        "    except IOError:\n",
        "        raise MyIOError(filename)\n",
        "    return d\n",
        "\n",
        "\n",
        "def export_trimmed_glove_vectors(vocab, glove_filename, trimmed_filename, dim):\n",
        "    \"\"\"Saves glove vectors in numpy array\n",
        "\n",
        "    Args:\n",
        "        vocab: dictionary vocab[word] = index\n",
        "        glove_filename: a path to a glove file\n",
        "        trimmed_filename: a path where to store a matrix in npy\n",
        "        dim: (int) dimension of embeddings\n",
        "\n",
        "    \"\"\"\n",
        "    embeddings = np.zeros([len(vocab), dim])\n",
        "    with open(glove_filename) as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(' ')\n",
        "            word = line[0]\n",
        "            embedding = [float(x) for x in line[1:]]\n",
        "            if word in vocab:\n",
        "                word_idx = vocab[word]\n",
        "                embeddings[word_idx] = np.asarray(embedding)\n",
        "\n",
        "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
        "\n",
        "\n",
        "def get_trimmed_glove_vectors(filename):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        filename: path to the npz file\n",
        "\n",
        "    Returns:\n",
        "        matrix of embeddings (np array)\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with np.load(filename) as data:\n",
        "            return data[\"embeddings\"]\n",
        "\n",
        "    except IOError:\n",
        "        raise MyIOError(filename)\n",
        "\n",
        "\n",
        "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
        "                    lowercase=False, chars=False, allow_unk=True):\n",
        "    \"\"\"Return lambda function that transform a word (string) into list,\n",
        "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
        "    its corresponding characters.\n",
        "\n",
        "    Args:\n",
        "        vocab: dict[word] = idx\n",
        "    Returns:\n",
        "        f(\"cat\") = ([12, 4, 32], 12345)\n",
        "                 = (list of char ids, word id)\n",
        "\n",
        "    \"\"\"\n",
        "    def f(word):\n",
        "        # 0. get chars of words\n",
        "        if vocab_chars is not None and chars == True:\n",
        "            char_ids = []\n",
        "            for char in word:\n",
        "                # ignore chars out of vocabulary\n",
        "                if char in vocab_chars:\n",
        "                    char_ids += [vocab_chars[char]]\n",
        "\n",
        "        # 1. preprocess word\n",
        "        if lowercase:\n",
        "            word = word.lower()\n",
        "        word = re.sub('\\d', '0', word)\n",
        "\n",
        "        # 2. get id of word\n",
        "        if vocab_words is not None:\n",
        "            if word in vocab_words:\n",
        "                word = vocab_words[word]\n",
        "            else:\n",
        "                if word.isdigit():\n",
        "                    word = vocab_words[NUM]\n",
        "                elif allow_unk:\n",
        "                    word = vocab_words[UNK]\n",
        "                else:\n",
        "                    print(word)\n",
        "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
        "                                    \"your vocab (tags?) is correct\")\n",
        "\n",
        "        # 3. return tuple char ids, word id\n",
        "        if vocab_chars is not None and chars == True:\n",
        "            return char_ids, word\n",
        "        else:\n",
        "            return word\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _pad_sequences(sequences, pad_tok, max_length):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sequences: a generator of list or tuple\n",
        "        pad_tok: the char to pad with\n",
        "    Returns:\n",
        "        a list of list where each sublist has same length\n",
        "    \"\"\"\n",
        "    sequence_padded, sequence_length = [], []\n",
        "\n",
        "    for seq in sequences:\n",
        "        seq = list(seq)\n",
        "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
        "        sequence_padded +=  [seq_]\n",
        "        sequence_length += [min(len(seq), max_length)]\n",
        "\n",
        "    return sequence_padded, sequence_length\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        sequences: a generator of list or tuple\n",
        "        pad_tok: the char to pad with\n",
        "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
        "    Returns:\n",
        "        a list of list where each sublist has same length\n",
        "\n",
        "    \"\"\"\n",
        "    if nlevels == 1:\n",
        "        max_length = max(map(lambda x : len(x), sequences))\n",
        "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
        "                                            pad_tok, max_length)\n",
        "\n",
        "    elif nlevels == 2:\n",
        "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
        "                               for seq in sequences])\n",
        "        sequence_padded, sequence_length = [], []\n",
        "        for seq in sequences:\n",
        "            # all words are same length now\n",
        "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
        "            sequence_padded += [sp]\n",
        "            sequence_length += [sl]\n",
        "\n",
        "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
        "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
        "                [pad_tok]*max_length_word, max_length_sentence)\n",
        "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
        "                max_length_sentence)\n",
        "\n",
        "    return sequence_padded, sequence_length\n",
        "\n",
        "\n",
        "def minibatches(data, minibatch_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data: generator of (sentence, tags) tuples\n",
        "        minibatch_size: (int)\n",
        "    Yields:\n",
        "        list of tuples\n",
        "\n",
        "    \"\"\"\n",
        "    x_batch, y_batch = [], []\n",
        "    for (x, y) in data:\n",
        "        if len(x_batch) == minibatch_size:\n",
        "            yield x_batch, y_batch\n",
        "            x_batch, y_batch = [], []\n",
        "\n",
        "        if type(x[0]) == tuple:\n",
        "            x = zip(*x)\n",
        "        x_batch += [x]\n",
        "        y_batch += [y]\n",
        "\n",
        "    if len(x_batch) != 0:\n",
        "        yield x_batch, y_batch\n",
        "\n",
        "\n",
        "def get_chunk_type(tok, idx_to_tag):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tok: id of token, ex 4\n",
        "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
        "    Returns:\n",
        "        tuple: \"B\", \"PER\"\n",
        "\n",
        "    \"\"\"\n",
        "    tag_name = idx_to_tag[tok]\n",
        "    tag_class = tag_name.split('-')[0]\n",
        "    tag_type = tag_name.split('-')[-1]\n",
        "    return tag_class, tag_type\n",
        "\n",
        "\n",
        "def get_chunks(seq, tags):\n",
        "    \"\"\"Given a sequence of tags, group entities and their position\n",
        "\n",
        "    Args:\n",
        "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
        "        tags: dict[\"O\"] = 4\n",
        "\n",
        "    Returns:\n",
        "        list of (chunk_type, chunk_start, chunk_end)\n",
        "    Example:\n",
        "        seq = [4, 5, 0, 3]\n",
        "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
        "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
        "\n",
        "    \"\"\"\n",
        "    default = tags[NONE]\n",
        "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "    chunks = []\n",
        "    chunk_type, chunk_start = None, None\n",
        "    for i, tok in enumerate(seq):\n",
        "        # End of a chunk 1\n",
        "        if tok == default and chunk_type is not None:\n",
        "            # Add a chunk.\n",
        "            chunk = (chunk_type, chunk_start, i)\n",
        "            chunks.append(chunk)\n",
        "            chunk_type, chunk_start = None, None\n",
        "\n",
        "        # End of a chunk + start of a chunk!\n",
        "        elif tok != default:\n",
        "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
        "            if chunk_type is None:\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
        "                chunk = (chunk_type, chunk_start, i)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # end condition\n",
        "    if chunk_type is not None:\n",
        "        chunk = (chunk_type, chunk_start, len(seq))\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def create_tag_dict(filename):\n",
        "    '''\n",
        "    From tags create dictionary grouping same classes\n",
        "    '''\n",
        "    # index to tag dic\n",
        "    indxToTag = dict()\n",
        "    with open(filename) as tags:\n",
        "        i = 0\n",
        "        for line in tags:\n",
        "            tag = line.rstrip('\\n')\n",
        "            if tag[0:2] in [\"B-\", \"I-\"]:\n",
        "                tag = tag[2:]\n",
        "            indxToTag[i] = tag\n",
        "            i +=1\n",
        "    return indxToTag"
      ],
      "id": "6rTIJHA_APMy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd9eNV1oAo_S"
      },
      "source": [
        "### 3.2 Avaliação"
      ],
      "id": "fd9eNV1oAo_S"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "smTgrcAxAruc"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
        "    if candidate == 'B-' + current_tag:\n",
        "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "                current_chunk[-1].append(current_pos - 1)\n",
        "        current_chunk.append([current_pos])\n",
        "    elif candidate == 'I-' + current_tag:\n",
        "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
        "            current_chunk.append([current_pos])\n",
        "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
        "            current_chunk.append([current_pos])\n",
        "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
        "        if len(current_chunk) > 0:\n",
        "            current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _update_last_chunk(current_chunk, current_pos):\n",
        "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "        current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _tag_precision_recall_f1(tp, fp, fn):\n",
        "    precision, recall, f1 = 0, 0, 0\n",
        "    if tp + fp > 0:\n",
        "        precision = tp / (tp + fp) * 100\n",
        "    if tp + fn > 0:\n",
        "        recall = tp / (tp + fn) * 100\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def _aggregate_metrics(results, total_correct):\n",
        "    total_true_entities = 0\n",
        "    total_predicted_entities = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    for tag, tag_metrics in results.items():\n",
        "        n_pred = tag_metrics['n_predicted_entities']\n",
        "        n_true = tag_metrics['n_true_entities']\n",
        "        total_true_entities += n_true\n",
        "        total_predicted_entities += n_pred\n",
        "        total_precision += tag_metrics['precision'] * n_pred\n",
        "        total_recall += tag_metrics['recall'] * n_true\n",
        "    \n",
        "    accuracy = 0\n",
        "    if total_true_entities > 0:\n",
        "        accuracy = total_correct / total_true_entities * 100\n",
        "    else:\n",
        "        print('CAUTION! Accuracy equals zero because there are no '\\\n",
        "              'correct entities. Check the correctness of your data.')\n",
        "    if total_predicted_entities > 0:\n",
        "        total_precision = total_precision / total_predicted_entities\n",
        "    total_recall = total_recall / total_true_entities\n",
        "    if total_precision + total_recall > 0:\n",
        "        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
        "    return total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy\n",
        "\n",
        "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
        "    print('processed {len} tokens ' \\\n",
        "          'with {tot_true} phrases; ' \\\n",
        "          'found: {tot_pred} phrases; ' \\\n",
        "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
        "                                         tot_true=total_true_entities,\n",
        "                                         tot_pred=total_predicted_entities,\n",
        "                                         tot_cor=total_correct))\n",
        "\n",
        "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
        "    print('precision:  {tot_prec:.2f}%; ' \\\n",
        "          'recall:  {tot_recall:.2f}%; ' \\\n",
        "          'F1:  {tot_f1:.2f}\\n'.format(acc=accuracy,\n",
        "                                           tot_prec=total_precision,\n",
        "                                           tot_recall=total_recall,\n",
        "                                           tot_f1=total_f1))\n",
        "\n",
        "def _print_tag_metrics(tag, tag_results):\n",
        "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
        "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
        "                               'F1:  {tot_f1:6.2f}; ' \\\n",
        "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
        "                                                                         tot_recall=tag_results['recall'],\n",
        "                                                                         tot_f1=tag_results['f1'],\n",
        "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
        "    # Find all tags\n",
        "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
        "\n",
        "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
        "    n_tokens = len(y_true)\n",
        "    total_correct = 0\n",
        "\n",
        "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
        "    # For each chunk we store starting and ending indices\n",
        "    for tag in tags:\n",
        "        true_chunk = list()\n",
        "        predicted_chunk = list()\n",
        "        for position in range(n_tokens):\n",
        "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
        "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
        "\n",
        "        _update_last_chunk(true_chunk, position)\n",
        "        _update_last_chunk(predicted_chunk, position)\n",
        "\n",
        "        # Then we find all correctly classified intervals\n",
        "        # True positive results\n",
        "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
        "        total_correct += tp\n",
        "\n",
        "        # And then just calculate errors of the first and second kind\n",
        "        # False negative\n",
        "        fn = len(true_chunk) - tp\n",
        "        # False positive\n",
        "        fp = len(predicted_chunk) - tp\n",
        "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
        "\n",
        "        results[tag]['precision'] = precision\n",
        "        results[tag]['recall'] = recall\n",
        "        results[tag]['f1'] = f1\n",
        "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
        "        results[tag]['n_true_entities'] = len(true_chunk)\n",
        "\n",
        "    total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
        "\n",
        "    if print_results:\n",
        "        _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
        "        _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
        "\n",
        "        if not short_report:\n",
        "            for tag, tag_results in results.items():\n",
        "                _print_tag_metrics(tag, tag_results)\n",
        "    return results, total_f1"
      ],
      "id": "smTgrcAxAruc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQhrN-IFA3Ja"
      },
      "source": [
        "### 3.3. Utils"
      ],
      "id": "WQhrN-IFA3Ja"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DYymx8LoA8Zr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_logger(filename):\n",
        "    \"\"\"Return a logger instance that writes in filename\n",
        "\n",
        "    Args:\n",
        "        filename: (string) path to log.txt\n",
        "\n",
        "    Returns:\n",
        "        logger: (instance of logger)\n",
        "\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger('logger')\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
        "    handler = logging.FileHandler(filename)\n",
        "    handler.setLevel(logging.DEBUG)\n",
        "    handler.setFormatter(logging.Formatter(\n",
        "            '%(asctime)s:%(levelname)s: %(message)s'))\n",
        "    logging.getLogger().addHandler(handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "class Progbar(object):\n",
        "    \"\"\"Progbar class copied from keras (https://github.com/fchollet/keras/)\n",
        "\n",
        "    Displays a progress bar.\n",
        "    Small edit : added strict arg to update\n",
        "    # Arguments\n",
        "        target: Total number of steps expected.\n",
        "        interval: Minimum visual progress update interval (in seconds).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target, width=30, verbose=1):\n",
        "        self.width = width\n",
        "        self.target = target\n",
        "        self.sum_values = {}\n",
        "        self.unique_values = []\n",
        "        self.start = time.time()\n",
        "        self.total_width = 0\n",
        "        self.seen_so_far = 0\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def update(self, current, values=[], exact=[], strict=[]):\n",
        "        \"\"\"\n",
        "        Updates the progress bar.\n",
        "        # Arguments\n",
        "            current: Index of current step.\n",
        "            values: List of tuples (name, value_for_last_step).\n",
        "                The progress bar will display averages for these values.\n",
        "            exact: List of tuples (name, value_for_last_step).\n",
        "                The progress bar will display these values directly.\n",
        "        \"\"\"\n",
        "\n",
        "        for k, v in values:\n",
        "            if k not in self.sum_values:\n",
        "                self.sum_values[k] = [v * (current - self.seen_so_far),\n",
        "                                      current - self.seen_so_far]\n",
        "                self.unique_values.append(k)\n",
        "            else:\n",
        "                self.sum_values[k][0] += v * (current - self.seen_so_far)\n",
        "                self.sum_values[k][1] += (current - self.seen_so_far)\n",
        "        for k, v in exact:\n",
        "            if k not in self.sum_values:\n",
        "                self.unique_values.append(k)\n",
        "            self.sum_values[k] = [v, 1]\n",
        "\n",
        "        for k, v in strict:\n",
        "            if k not in self.sum_values:\n",
        "                self.unique_values.append(k)\n",
        "            self.sum_values[k] = v\n",
        "\n",
        "        self.seen_so_far = current\n",
        "\n",
        "        now = time.time()\n",
        "        if self.verbose == 1:\n",
        "            prev_total_width = self.total_width\n",
        "            sys.stdout.write(\"\\b\" * prev_total_width)\n",
        "            sys.stdout.write(\"\\r\")\n",
        "\n",
        "            numdigits = int(np.floor(np.log10(self.target))) + 1\n",
        "            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n",
        "            bar = barstr % (current, self.target)\n",
        "            prog = float(current)/self.target\n",
        "            prog_width = int(self.width*prog)\n",
        "            if prog_width > 0:\n",
        "                bar += ('='*(prog_width-1))\n",
        "                if current < self.target:\n",
        "                    bar += '>'\n",
        "                else:\n",
        "                    bar += '='\n",
        "            bar += ('.'*(self.width-prog_width))\n",
        "            bar += ']'\n",
        "            sys.stdout.write(bar)\n",
        "            self.total_width = len(bar)\n",
        "\n",
        "            if current:\n",
        "                time_per_unit = (now - self.start) / current\n",
        "            else:\n",
        "                time_per_unit = 0\n",
        "            eta = time_per_unit*(self.target - current)\n",
        "            info = ''\n",
        "            if current < self.target:\n",
        "                info += ' - ETA: %ds' % eta\n",
        "            else:\n",
        "                info += ' - %ds' % (now - self.start)\n",
        "            for k in self.unique_values:\n",
        "                if type(self.sum_values[k]) is list:\n",
        "                    info += ' - %s: %.4f' % (k,\n",
        "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
        "                else:\n",
        "                    info += ' - %s: %s' % (k, self.sum_values[k])\n",
        "\n",
        "            self.total_width += len(info)\n",
        "            if prev_total_width > self.total_width:\n",
        "                info += ((prev_total_width-self.total_width) * \" \")\n",
        "\n",
        "            sys.stdout.write(info)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            if current >= self.target:\n",
        "                sys.stdout.write(\"\\n\")\n",
        "\n",
        "        if self.verbose == 2:\n",
        "            if current >= self.target:\n",
        "                info = '%ds' % (now - self.start)\n",
        "                for k in self.unique_values:\n",
        "                    info += ' - %s: %.4f' % (k,\n",
        "                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n",
        "                sys.stdout.write(info + \"\\n\")\n",
        "\n",
        "    def add(self, n, values=[]):\n",
        "        self.update(self.seen_so_far+n, values)\n",
        "\n",
        "\n"
      ],
      "id": "DYymx8LoA8Zr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nbIE-bsBCo1"
      },
      "source": [
        "### 3.4 Base_model"
      ],
      "id": "8nbIE-bsBCo1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qmH7hMVgBPPg"
      },
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Defines self.config and self.logger\n",
        "\n",
        "        Args:\n",
        "            config: (Config instance) class with hyper parameters,\n",
        "                vocab and embeddings\n",
        "\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = config.logger\n",
        "        self.sess   = None\n",
        "        self.saver  = None\n",
        "\n",
        "\n",
        "    def reinitialize_weights(self, scope_name):\n",
        "        \"\"\"Reinitializes the weights of a given layer\"\"\"\n",
        "        variables = tf.contrib.framework.get_variables(scope_name)\n",
        "        init = tf.variables_initializer(variables)\n",
        "        self.sess.run(init)\n",
        "\n",
        "\n",
        "    def add_train_op(self, lr_method, lr, loss, clip=-1):\n",
        "        \"\"\"Defines self.train_op that performs an update on a batch\n",
        "\n",
        "        Args:\n",
        "            lr_method: (string) sgd method, for example \"adam\"\n",
        "            lr: (tf.placeholder) tf.float32, learning rate\n",
        "            loss: (tensor) tf.float32 loss to minimize\n",
        "            clip: (python float) clipping of gradient. If < 0, no clipping\n",
        "\n",
        "        \"\"\"\n",
        "        _lr_m = lr_method.lower() # lower to make sure\n",
        "\n",
        "        with tf.variable_scope(\"train_step\"):\n",
        "            if _lr_m == 'adam': # sgd method\n",
        "                optimizer = tf.train.AdamOptimizer(lr)\n",
        "            elif _lr_m == 'adagrad':\n",
        "                optimizer = tf.train.AdagradOptimizer(lr)\n",
        "            elif _lr_m == 'sgd':\n",
        "                optimizer = tf.train.MomentumOptimizer(lr, 0.9)\n",
        "            elif _lr_m == 'rmsprop':\n",
        "                optimizer = tf.train.RMSPropOptimizer(lr)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
        "\n",
        "            if clip > 0: # gradient clipping if clip is positive\n",
        "                grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
        "                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
        "                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n",
        "            else:\n",
        "                self.train_op = optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "    def initialize_session(self):\n",
        "        \"\"\"Defines self.sess and initialize the variables\"\"\"\n",
        "        self.logger.info(\"Initializing tf session\")\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "    def restore_session(self, dir_model):\n",
        "        \"\"\"Reload weights into session\n",
        "\n",
        "        Args:\n",
        "            sess: tf.Session()\n",
        "            dir_model: dir with weights\n",
        "\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Reloading the latest trained model...\")\n",
        "        self.saver.restore(self.sess, dir_model)\n",
        "\n",
        "\n",
        "    def save_session(self):\n",
        "        \"\"\"Saves session = weights\"\"\"\n",
        "        if not os.path.exists(self.config.dir_model):\n",
        "            os.makedirs(self.config.dir_model)\n",
        "        self.saver.save(self.sess, self.config.dir_model)\n",
        "\n",
        "\n",
        "    def close_session(self):\n",
        "        \"\"\"Closes the session\"\"\"\n",
        "        self.sess.close()\n",
        "\n",
        "\n",
        "    def add_summary(self):\n",
        "        \"\"\"Defines variables for Tensorboard\n",
        "\n",
        "        Args:\n",
        "            dir_output: (string) where the results are written\n",
        "\n",
        "        \"\"\"\n",
        "        self.merged      = tf.summary.merge_all()\n",
        "        self.file_writer = tf.summary.FileWriter(self.config.dir_output,\n",
        "                self.sess.graph)\n",
        "\n",
        "\n",
        "    def train(self, train, dev, best_score=0):\n",
        "        \"\"\"Performs training with early stopping and lr exponential decay\n",
        "\n",
        "        Args:\n",
        "            train: dataset that yields tuple of (sentences, tags)\n",
        "            dev: dataset\n",
        "\n",
        "        \"\"\"\n",
        "        nepoch_no_imprv = 0 # for early stopping\n",
        "        self.add_summary() # tensorboard        \n",
        "        for epoch in range(self.config.nepochs): \n",
        "            self.logger.info(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
        "                        self.config.nepochs))\n",
        "\n",
        "            score = self.run_epoch(train, dev, epoch)\n",
        "            self.config.lr *= self.config.lr_decay # decay learning rate\n",
        "\n",
        "            # early stopping and saving best parameters\n",
        "            if score >= best_score:\n",
        "                nepoch_no_imprv = 0\n",
        "                self.save_session()\n",
        "                best_score = score\n",
        "                self.logger.info(\"- new best score!\")\n",
        "            else:\n",
        "                nepoch_no_imprv += 1\n",
        "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
        "                    self.logger.info(\"- early stopping {} epochs without \"\\\n",
        "                            \"improvement\".format(nepoch_no_imprv))\n",
        "                    break\n",
        "\n",
        "\n",
        "    def evaluate(self, test):\n",
        "        \"\"\"Evaluate model on test set\n",
        "\n",
        "        Args:\n",
        "            test: instance of class Dataset\n",
        "\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Testing model over test set\")\n",
        "        metrics = self.run_evaluate(test)\n",
        "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
        "                for k, v in metrics.items()])\n",
        "        self.logger.info(msg)\n"
      ],
      "id": "qmH7hMVgBPPg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KB0hNsmBYGP"
      },
      "source": [
        "### 3.5 NER model"
      ],
      "id": "6KB0hNsmBYGP"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_9P-JC2UBZTD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code modified by the authors of leNER-Br paper:\n",
        "    modify run_evaluate to base score on token correctness\n",
        "'''\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "class NERModel(BaseModel):\n",
        "    \"\"\"Specialized class of Model for NER\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(NERModel, self).__init__(config)\n",
        "        self.idx_to_tag = {idx: tag for tag, idx in\n",
        "                           self.config.vocab_tags.items()}\n",
        "\n",
        "\n",
        "    def add_placeholders(self):\n",
        "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
        "        # shape = (batch size, max length of sentence in batch)\n",
        "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
        "                        name=\"word_ids\")\n",
        "\n",
        "        # shape = (batch size)\n",
        "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                        name=\"sequence_lengths\")\n",
        "\n",
        "        # shape = (batch size, max length of sentence, max length of word)\n",
        "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n",
        "                        name=\"char_ids\")\n",
        "\n",
        "        # shape = (batch_size, max_length of sentence)\n",
        "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None],\n",
        "                        name=\"word_lengths\")\n",
        "\n",
        "        # shape = (batch size, max length of sentence in batch)\n",
        "        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n",
        "                        name=\"labels\")\n",
        "\n",
        "        # hyper parameters\n",
        "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
        "                        name=\"dropout\")\n",
        "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
        "                        name=\"lr\")\n",
        "\n",
        "\n",
        "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
        "        \"\"\"Given some data, pad it and build a feed dictionary\n",
        "\n",
        "        Args:\n",
        "            words: list of sentences. A sentence is a list of ids of a list of\n",
        "                words. A word is a list of ids\n",
        "            labels: list of ids\n",
        "            lr: (float) learning rate\n",
        "            dropout: (float) keep prob\n",
        "\n",
        "        Returns:\n",
        "            dict {placeholder: value}\n",
        "\n",
        "        \"\"\"\n",
        "        # perform padding of the given data\n",
        "        if self.config.use_chars:\n",
        "            char_ids, word_ids = zip(*words)\n",
        "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
        "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
        "                nlevels=2)\n",
        "        else:\n",
        "            word_ids, sequence_lengths = pad_sequences(words, 0)\n",
        "\n",
        "        # build feed dictionary\n",
        "        feed = {\n",
        "            self.word_ids: word_ids,\n",
        "            self.sequence_lengths: sequence_lengths\n",
        "        }\n",
        "\n",
        "        if self.config.use_chars:\n",
        "            feed[self.char_ids] = char_ids\n",
        "            feed[self.word_lengths] = word_lengths\n",
        "\n",
        "        if labels is not None:\n",
        "            labels, _ = pad_sequences(labels, 0)\n",
        "            feed[self.labels] = labels\n",
        "\n",
        "        if lr is not None:\n",
        "            feed[self.lr] = lr\n",
        "\n",
        "        if dropout is not None:\n",
        "            feed[self.dropout] = dropout\n",
        "\n",
        "        return feed, sequence_lengths\n",
        "\n",
        "\n",
        "    def add_word_embeddings_op(self):\n",
        "        \"\"\"Defines self.word_embeddings\n",
        "\n",
        "        If self.config.embeddings is not None and is a np array initialized\n",
        "        with pre-trained word vectors, the word embeddings is just a look-up\n",
        "        and we don't train the vectors. Otherwise, a random matrix with\n",
        "        the correct shape is initialized.\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(\"words\"):\n",
        "            if self.config.embeddings is None:\n",
        "                self.logger.info(\"WARNING: randomly initializing word vectors\")\n",
        "                _word_embeddings = tf.get_variable(\n",
        "                        name=\"_word_embeddings\",\n",
        "                        dtype=tf.float32,\n",
        "                        shape=[self.config.nwords, self.config.dim_word])\n",
        "            else:\n",
        "                _word_embeddings = tf.Variable(\n",
        "                        self.config.embeddings,\n",
        "                        name=\"_word_embeddings\",\n",
        "                        dtype=tf.float32,\n",
        "                        trainable=self.config.train_embeddings)\n",
        "\n",
        "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
        "                    self.word_ids, name=\"word_embeddings\")\n",
        "\n",
        "        with tf.variable_scope(\"chars\"):            \n",
        "            if self.config.use_chars:\n",
        "                # get char embeddings matrix\n",
        "                _char_embeddings = tf.get_variable(\n",
        "                        name=\"_char_embeddings\",\n",
        "                        dtype=tf.float32,\n",
        "                        shape=[self.config.nchars, self.config.dim_char])\n",
        "                char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
        "                        self.char_ids, name=\"char_embeddings\")\n",
        "               \n",
        "\n",
        "                # put the time dimension on axis=1\n",
        "                s = tf.shape(char_embeddings)\n",
        "                char_embeddings = tf.reshape(char_embeddings,\n",
        "                        shape=[s[0]*s[1], s[-2], self.config.dim_char])\n",
        "                word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n",
        "\n",
        "                # bi lstm on chars\n",
        "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
        "                        state_is_tuple=True)\n",
        "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
        "                        state_is_tuple=True)\n",
        "                _output = tf.nn.bidirectional_dynamic_rnn(\n",
        "                        cell_fw, cell_bw, char_embeddings,\n",
        "                        sequence_length=word_lengths, dtype=tf.float32)\n",
        "\n",
        "                # read and concat output\n",
        "                _, ((_, output_fw), (_, output_bw)) = _output\n",
        "                output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "\n",
        "                # shape = (batch size, max sentence length, char hidden size)\n",
        "                output = tf.reshape(output,\n",
        "                        shape=[s[0], s[1], 2*self.config.hidden_size_char])\n",
        "                word_embeddings = tf.concat([word_embeddings, output], axis=-1)\n",
        "\n",
        "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
        "\n",
        "\n",
        "    def add_logits_op(self):\n",
        "        \"\"\"Defines self.logits\n",
        "\n",
        "        For each word in each sentence of the batch, it corresponds to a vector\n",
        "        of scores, of dimension equal to the number of tags.\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(\"bi-lstm\"):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
        "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                    cell_fw, cell_bw, self.word_embeddings,\n",
        "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
        "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
        "            output = tf.nn.dropout(output, self.dropout)\n",
        "\n",
        "        with tf.variable_scope(\"proj\"):\n",
        "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
        "                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n",
        "\n",
        "            b = tf.get_variable(\"b\", shape=[self.config.ntags],\n",
        "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
        "\n",
        "            nsteps = tf.shape(output)[1]\n",
        "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
        "            pred = tf.matmul(output, W) + b\n",
        "            self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
        "\n",
        "\n",
        "    def add_pred_op(self):\n",
        "        \"\"\"Defines self.labels_pred\n",
        "\n",
        "        This op is defined only in the case where we don't use a CRF since in\n",
        "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
        "        functions in other words). With theCRF, as the inference is coded\n",
        "        in python and not in pure tensroflow, we have to make the prediciton\n",
        "        outside the graph.\n",
        "        \"\"\"\n",
        "        if not self.config.use_crf:\n",
        "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
        "                    tf.int32)\n",
        "\n",
        "\n",
        "    def add_loss_op(self):\n",
        "        \"\"\"Defines the loss\"\"\"\n",
        "        if self.config.use_crf:\n",
        "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
        "                    self.logits, self.labels, self.sequence_lengths)\n",
        "            self.trans_params = trans_params # need to evaluate it for decoding\n",
        "            self.loss = tf.reduce_mean(-log_likelihood)\n",
        "        else:\n",
        "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits, labels=self.labels)\n",
        "            mask = tf.sequence_mask(self.sequence_lengths)\n",
        "            losses = tf.boolean_mask(losses, mask)\n",
        "            self.loss = tf.reduce_mean(losses)\n",
        "\n",
        "        # for tensorboard\n",
        "        tf.summary.scalar(\"loss\", self.loss)\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        # NER specific functions\n",
        "        self.add_placeholders()\n",
        "        self.add_word_embeddings_op()\n",
        "        self.add_logits_op()\n",
        "        self.add_pred_op()\n",
        "        self.add_loss_op()\n",
        "\n",
        "        # Generic functions that add training op and initialize session\n",
        "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
        "                self.config.clip)\n",
        "        self.initialize_session() # now self.sess is defined and vars are init\n",
        "\n",
        "\n",
        "    def predict_batch(self, words):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            words: list of sentences\n",
        "\n",
        "        Returns:\n",
        "            labels_pred: list of labels for each sentence\n",
        "            sequence_length\n",
        "\n",
        "        \"\"\"\n",
        "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
        "\n",
        "        if self.config.use_crf:\n",
        "            # get tag scores and transition params of CRF\n",
        "            viterbi_sequences = []\n",
        "            logits, trans_params = self.sess.run(\n",
        "                    [self.logits, self.trans_params], feed_dict=fd)\n",
        "\n",
        "            # iterate over the sentences because no batching in vitervi_decode\n",
        "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
        "                logit = logit[:sequence_length] # keep only the valid steps\n",
        "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
        "                        logit, trans_params)\n",
        "                viterbi_sequences += [viterbi_seq]\n",
        "\n",
        "            return viterbi_sequences, sequence_lengths\n",
        "\n",
        "        else:\n",
        "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
        "\n",
        "            return labels_pred, sequence_lengths\n",
        "\n",
        "\n",
        "    def run_epoch(self, train, dev, epoch):\n",
        "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
        "\n",
        "        Args:\n",
        "            train: dataset that yields tuple of sentences, tags\n",
        "            dev: dataset\n",
        "            epoch: (int) index of the current epoch\n",
        "\n",
        "        Returns:\n",
        "            f1: (python float), score to select model on, higher is better\n",
        "\n",
        "        \"\"\"\n",
        "        # progbar stuff for logging\n",
        "        batch_size = self.config.batch_size\n",
        "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
        "        prog = Progbar(target=nbatches)\n",
        "\n",
        "        # iterate over dataset\n",
        "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
        "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
        "                    self.config.dropout)\n",
        "\n",
        "            _, train_loss, summary = self.sess.run(\n",
        "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
        "\n",
        "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
        "\n",
        "            # tensorboard\n",
        "            if i % 10 == 0:\n",
        "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
        "\n",
        "        metrics = self.run_evaluate(dev)\n",
        "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
        "                for k, v in metrics.items()])\n",
        "        self.logger.info(msg)\n",
        "\n",
        "        return metrics[\"f1\"]\n",
        "\n",
        "\n",
        "    def run_evaluate(self, test, short_report=False):\n",
        "        \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "        preds = []\n",
        "        labels = []\n",
        "\n",
        "        for words, labs in minibatches(test, self.config.batch_size):\n",
        "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
        "\n",
        "            for lab, lab_pred, length in zip(labs, labels_pred,\n",
        "                                             sequence_lengths):\n",
        "                lab_pred = lab_pred[:length]\n",
        "                lab = lab[:length]\n",
        "                preds.append(lab_pred)\n",
        "                labels.append(lab)\n",
        "        preds = [self.idx_to_tag[item] for items in preds for item in items]\n",
        "        labels = [self.idx_to_tag[item] for items in labels for item in items]\n",
        "\n",
        "        _, f1= precision_recall_f1(labels, preds, print_results=True, short_report=short_report)\n",
        "        acc = accuracy_score(preds, labels)\n",
        "\n",
        "        return {\"acc\": 100*acc, \"f1\": f1}\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, words_raw):\n",
        "        \"\"\"Returns list of tags\n",
        "\n",
        "        Args:\n",
        "            words_raw: list of words (string), just one sentence (no batch)\n",
        "\n",
        "        Returns:\n",
        "            preds: list of tags (string), one for each word in the sentence\n",
        "\n",
        "        \"\"\"\n",
        "        words = [self.config.processing_word(w) for w in words_raw]\n",
        "        if type(words[0]) == tuple:\n",
        "            words = zip(*words)\n",
        "        pred_ids, _ = self.predict_batch([words])\n",
        "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
        "\n",
        "        return preds\n"
      ],
      "id": "_9P-JC2UBZTD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDw9i-vfBmJe"
      },
      "source": [
        "### 3.6 Config do Modelo"
      ],
      "id": "qDw9i-vfBmJe"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QmeYgrK0Bn6Y"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Configs set by the authors of leNER-Br paper\n",
        "'''\n",
        "\n",
        "class Config():\n",
        "    def __init__(self, load=True):\n",
        "        \"\"\"Initialize hyperparameters and load vocabs\n",
        "\n",
        "        Args:\n",
        "            load_embeddings: (bool) if True, load embeddings into\n",
        "                np array, else None\n",
        "\n",
        "        \"\"\"\n",
        "        # directory for training outputs\n",
        "        if not os.path.exists(self.dir_output):\n",
        "            os.makedirs(self.dir_output)\n",
        "\n",
        "        # create instance of logger\n",
        "        self.logger = get_logger(self.path_log)\n",
        "\n",
        "        # load if requested (default)\n",
        "        if load:\n",
        "            self.load()        \n",
        "\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Loads vocabulary, processing functions and embeddings\n",
        "\n",
        "        Supposes that build_data.py has been run successfully and that\n",
        "        the corresponding files have been created (vocab and trimmed GloVe\n",
        "        vectors)\n",
        "\n",
        "        \"\"\"\n",
        "        # 1. vocabulary\n",
        "        self.vocab_words = load_vocab(self.filename_words)\n",
        "        self.vocab_tags  = load_vocab(self.filename_tags)\n",
        "        self.vocab_chars = load_vocab(self.filename_chars)\n",
        "\n",
        "        self.nwords     = len(self.vocab_words)\n",
        "        self.nchars     = len(self.vocab_chars)\n",
        "        self.ntags      = len(self.vocab_tags)\n",
        "\n",
        "        # 2. get processing functions that map str -> id\n",
        "        self.processing_word = get_processing_word(self.vocab_words,\n",
        "                self.vocab_chars, lowercase=True, chars=self.use_chars)\n",
        "        self.processing_tag  = get_processing_word(self.vocab_tags,\n",
        "                lowercase=False, allow_unk=False)\n",
        "\n",
        "        # 3. get pre-trained embeddings\n",
        "        self.embeddings = (get_trimmed_glove_vectors(self.filename_trimmed)\n",
        "                if self.use_pretrained else None)\n",
        "\n",
        "\n",
        "    # general config\n",
        "    dir_output = DIR+\"results/prototype_revised/\"\n",
        "    dir_model  = dir_output + \"model.weights/\"\n",
        "    path_log   = dir_output + \"log.txt\"    \n",
        "\n",
        "\n",
        "    # embeddings\n",
        "    dim_word = 300\n",
        "    dim_char = 50\n",
        "\n",
        "    # glove files #glove_s300.txt <-- original\n",
        "    filename_glove = \"/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_LenerBr/data/glove_s300.txt\" #[glove_s300.txt, word2vec_s300.txt, fasttext_s300.txt, wang2vec_s300.txt]\n",
        "    # trimmed embeddings (created from glove_filename with build_data.py)\n",
        "    filename_trimmed = DIR+\"glove.6B.{}d.trimmed.npz\".format(dim_word)\n",
        "    use_pretrained = True\n",
        "\n",
        "    # dataset\n",
        "    # filename_dev = \"data/coNLL/eng/eng.testa.iob\"\n",
        "    # filename_test = \"data/coNLL/eng/eng.testb.iob\"\n",
        "    # filename_train = \"data/coNLL/eng/eng.train.iob\"\n",
        "    \n",
        "    #berg atualizar\n",
        "    dataset_dir = DIR\n",
        "    \n",
        "    filename_dev = dataset_dir+\"dev.txt\"\n",
        "    filename_test = dataset_dir+\"test.txt\"\n",
        "    filename_train = dataset_dir+\"train.txt\"\n",
        "\n",
        "    max_iter = None # if not None, max number of examples in Dataset\n",
        "\n",
        "    # vocab (created from dataset with build_data.py)\n",
        "    filename_words = dataset_dir+\"words.txt\"\n",
        "    filename_tags = dataset_dir+\"tags.txt\"\n",
        "    filename_chars = dataset_dir+\"chars.txt\"\n",
        "\n",
        "    '''\n",
        "    # training\n",
        "    #parâmetros originais\n",
        "    train_embeddings = True\n",
        "    nepochs          = 55\n",
        "    dropout          = 0.5\n",
        "    batch_size       = 10\n",
        "    lr_method        = \"sgd\"\n",
        "    lr               = 0.015\n",
        "    lr_decay         = 0.95\n",
        "    clip             = 5 # if negative, no clipping\n",
        "    nepoch_no_imprv  = 20\n",
        "    '''\n",
        "\n",
        "    # training\n",
        "    train_embeddings = True\n",
        "    nepochs          = -1 #<------ [25, 35, 55, 75]    \n",
        "    batch_size       = -1 # 10 #<------ [10, 20, 40, 60]\n",
        "    lr_method        = -1 #\"sgd\" #Optimizer <------- ['adam', 'adagrad', 'sgd', 'rmsprop']\t\n",
        "    dropout          = 0.5 \n",
        "    lr               = 0.015 #Learning rate\n",
        "    lr_decay         = 0.95 #Learning rate decay\n",
        "    clip             = 5 # Gradient clipping threshold. if negative, no clipping\n",
        "    nepoch_no_imprv  = 1000 #number of epochs without improment = 20 <-- padrão original. alterado para numero grande para rodar todas as epocas\n",
        "\n",
        "    # model hyperparameters\n",
        "    hidden_size_char = 25 #25 # lstm on chars - First LSTM layer hidden units <----- [15, 25, 35, 45]\n",
        "    hidden_size_lstm = 100 # lstm on word embeddings - Second LSTM layer hidden units\n",
        "\n",
        "    # NOTE: if both chars and crf, only 1.6x slower on GPU\n",
        "    use_crf = True # if crf, training is 1.7x slower on CPU\n",
        "    use_chars = True # if char embedding, training is 3.5x slower on CPU\n",
        " "
      ],
      "id": "QmeYgrK0Bn6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WelIy4K2sPW"
      },
      "source": [
        "### 3.7 Build_data\n",
        "ATENÇÃO: só deve ser executado num novo dataset, para extrair informações de vocabulkarios, tags e caracteres."
      ],
      "id": "2WelIy4K2sPW"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_VsvV972_Xr",
        "outputId": "ba3abe59-faa3-4f36-a0de-1472c287258e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rodada:  /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_LenerBr/data/avaliacao_hyper/lener/\n",
            "\n",
            "--> dev\n",
            "--> test\n",
            "--> train\n",
            "Building vocab...\n",
            "- done. 7639 tokens\n",
            "Building vocab...\n",
            "- done. 929606 tokens\n",
            "--> vocab train\n",
            "Writing vocab...\n",
            "- done. 7021 tokens\n",
            "--> vocab tags dev\n",
            "Writing vocab...\n",
            "- done. 13 tokens\n",
            "Writing vocab...\n",
            "- done. 129 tokens\n"
          ]
        }
      ],
      "source": [
        "# This file was developed as part of the project reported in the paper below.\n",
        "# We request that users cite our paper in any publication that is generated\n",
        "# as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "def main():\n",
        "    \"\"\"Procedure to build data\n",
        "    You MUST RUN this procedure. It iterates over the whole dataset (train,\n",
        "    dev and test) and extract the vocabularies in terms of words, tags, and\n",
        "    characters. Having built the vocabularies it writes them in a file. The\n",
        "    writing of vocabulary in a file assigns an id (the line #) to each word.\n",
        "    It then extract the relevant GloVe vectors and stores them in a np array\n",
        "    such that the i-th entry corresponds to the i-th word in the vocabulary.\n",
        "    Args:\n",
        "        config: (instance of Config) has attributes like hyper-params...\n",
        "    \"\"\"\n",
        "    # get config and processing of words\n",
        "    config = Config(load=False)\n",
        "    processing_word = get_processing_word(lowercase=True)\n",
        "\n",
        "    # Generators\n",
        "    print('--> dev')\n",
        "    dev   = CoNLLDataset(config.filename_dev, processing_word)\n",
        "    print('--> test')\n",
        "    test  = CoNLLDataset(config.filename_test, processing_word)\n",
        "    print('--> train')\n",
        "    train = CoNLLDataset(config.filename_train, processing_word)\n",
        "\n",
        "    # Build Word and Tag vocab\n",
        "    # vocab_words, vocab_tags = get_vocabs([train, dev, test, test2])\n",
        "    vocab_words, vocab_tags = get_vocabs([train, dev])\n",
        "    vocab_glove = get_glove_vocab(config.filename_glove)\n",
        "\n",
        "    vocab = vocab_words & vocab_glove\n",
        "    vocab.add(UNK)\n",
        "    vocab.add(NUM)\n",
        "\n",
        "    # Save vocab\n",
        "    print('--> vocab train')\n",
        "    write_vocab(vocab, config.filename_words)\n",
        "    print('--> vocab tags dev')\n",
        "    write_vocab(vocab_tags, config.filename_tags)\n",
        "\n",
        "    # Trim GloVe Vectors\n",
        "    vocab = load_vocab(config.filename_words)\n",
        "    export_trimmed_glove_vectors(vocab, config.filename_glove,\n",
        "                                config.filename_trimmed, config.dim_word)\n",
        "\n",
        "    # Build and save char vocab\n",
        "    train = CoNLLDataset(config.filename_train)\n",
        "    vocab_chars = get_char_vocab(train)\n",
        "    write_vocab(vocab_chars, config.filename_chars)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"rodada: \", DIR)\n",
        "    print()\n",
        "    main()\n"
      ],
      "id": "j_VsvV972_Xr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUOkBCRxB0VO"
      },
      "source": [
        "## 4. Treinamento\n",
        "To retrain the model from scratch"
      ],
      "id": "LUOkBCRxB0VO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQbK2i8KCCaQ"
      },
      "outputs": [],
      "source": [
        "# This file was used as part of the project reported in the paper below.\n",
        "# We kindly request that users cite our paper in any publication that is \n",
        "# generated as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "def main():    \n",
        "    rodada = 'Original Avaliação Lener Grid C28'\n",
        "    print(f'------------> inicio da rodada {rodada}<-----------------')\n",
        "    # create instance of config        \n",
        "    config = Config()\n",
        "    \n",
        "    #hyperparameters        \n",
        "    config.batch_size = 40          # original 10 teste = [10, 20, 40]\n",
        "    config.nepochs = 75             # original 55 teste = [35, 55, 75]\n",
        "    config.lr_method = 'rmsprop'    # original sgd teste = ['adam', 'adagrad', 'sgd', 'rmsprop']    \n",
        "    \n",
        "    print(f'Configuração: {rodada} - HY:\\nBatch_size:\\t{config.batch_size}\\nEpocas:\\t{config.nepochs}')\n",
        "    print(f'Otimizador:\\t{config.lr_method}\\nDropout:\\t{config.dropout}')\n",
        "    print(\"\\n----------------------------------------------\\n----------------------------------------------\")\n",
        "    \n",
        "    # build model\n",
        "    model = NERModel(config)\n",
        "    model.build()\n",
        "    score = 0\n",
        "    # model.restore_session(config.dir_model) # optional, restore weights\n",
        "    # model.reinitialize_weights(\"proj\")\n",
        "    # make sure to make score equals models best score\n",
        "\n",
        "    # create datasets\n",
        "    dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n",
        "                        config.processing_tag, config.max_iter)\n",
        "    train = CoNLLDataset(config.filename_train, config.processing_word,\n",
        "                        config.processing_tag, config.max_iter)\n",
        "\n",
        "    # train model\n",
        "    model.train(train, dev, score)\n",
        "    \n",
        "    print(f'------------> fim da rodada {rodada}<-----------------')\n",
        "    print(\"\\n----------------------------------------------\\n----------------------------------------------\")        \n",
        "    print(\"\\n==============================================\\n==============================================\")                \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    print(\"DIRETORIO PRINCIPAL ----> \", DIR)\n",
        "    print()\n",
        "    main()\n"
      ],
      "id": "gQbK2i8KCCaQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWd_IluuAvBb"
      },
      "source": [
        "\n",
        "# 5. Avaliação\n",
        "\n"
      ],
      "id": "mWd_IluuAvBb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrrQ-nsVJVDW"
      },
      "source": [
        "## 5.1 ClassScores.py\n",
        "To obtain the f1 scores (per token) for each class on each part of the dataset: train, dev or test\n"
      ],
      "id": "lrrQ-nsVJVDW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNdq_4LwCKbF",
        "outputId": "10526ee9-a825-4c63-b134-3cdea082ba2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "NumExpr defaulting to 4 threads.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-08e8ff91dea0>:146: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From <ipython-input-9-08e8ff91dea0>:146: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-08e8ff91dea0>:151: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From <ipython-input-9-08e8ff91dea0>:151: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-08e8ff91dea0>:162: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From <ipython-input-9-08e8ff91dea0>:162: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Initializing tf session\n",
            "Reloading the latest trained model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/PROPOR_LenerBr/data/pls_tipo/results/prototype_revised/model.weights/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Restoring parameters from /content/drive/MyDrive/PROPOR_LenerBr/data/pls_tipo/results/prototype_revised/model.weights/\n",
            "Results on /content/drive/MyDrive/PROPOR_LenerBr/data/pls_tipo/data/test.txt set\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATA     0.9679    1.0000    0.9837       181\n",
            "      EVENTO     1.0000    0.3103    0.4737        29\n",
            " ORGANIZACAO     0.0000    0.0000    0.0000         0\n",
            "PRODUTODELEI     0.0000    0.0000    0.0000         0\n",
            "       LOCAL     0.0000    0.0000    0.0000         0\n",
            "  FUNDAMENTO     0.0000    0.0000    0.0000         0\n",
            "      PESSOA     0.0000    0.0000    0.0000         0\n",
            "\n",
            "   micro avg     0.9694    0.9048    0.9360       210\n",
            "   macro avg     0.2811    0.1872    0.2082       210\n",
            "weighted avg     0.9723    0.9048    0.9133       210\n",
            "\n",
            "0.9359605911330049\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Code entirely written by the authors of leNER-Br paper\n",
        "'''\n",
        "\n",
        "# This file was developed as part of the project reported in the paper below.\n",
        "# We kindly request that users cite our paper in any publication that is \n",
        "# generated as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "\n",
        "#from model.data_utils import CoNLLDataset\n",
        "#from model.ner_model import NERModel\n",
        "#from model.config import Config\n",
        "#from model.data_utils import minibatches, pad_sequences, get_chunks, create_tag_dict\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "#import numpy as np\n",
        "#import sys\n",
        "\n",
        "\n",
        "def classScores(model, test):\n",
        "        \"\"\"Evaluates performance on test set\n",
        "\n",
        "        Args:\n",
        "            test: dataset that yields tuple of (sentences, tags)\n",
        "\n",
        "        Returns:\n",
        "            metrics: (dict) metrics[\"f1_<label>\"] = 98.4, ...\n",
        "\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        labels = []\n",
        "        for words, labs in minibatches(test, model.config.batch_size):\n",
        "            labels_pred, sequence_lengths = model.predict_batch(words)\n",
        "\n",
        "            for lab, lab_pred, length in zip(labs, labels_pred,\n",
        "                                             sequence_lengths):\n",
        "                lab_pred = lab_pred[:length]\n",
        "                lab = lab[:length]\n",
        "                preds.append(lab_pred)\n",
        "                labels.append(lab)\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "\n",
        "def main(dataset, config):\n",
        "\n",
        "    # build model\n",
        "    model = NERModel(config)\n",
        "    model.build()\n",
        "    model.restore_session(config.dir_model)\n",
        "\n",
        "    # index to tag dic\n",
        "    indxToTag = create_tag_dict(DIR+\"/data/tags.txt\")\n",
        "    #berg\n",
        "    labels_lener = ['JURISPRUDENCIA', 'LOCAL', 'TEMPO',  'PESSOA', 'LEGISLACAO', 'ORGANIZACAO']\n",
        "\n",
        "    labels_projeto_tipo = ['DATA', 'EVENTO', \n",
        "                           'FUNDapelido', 'FUNDlei', 'FUNDprojetodelei', 'FUNDsolicitacaotrabalho', \n",
        "                           'LOCALconcreto', 'LOCALvirtual', \n",
        "                           'ORGgovernamental', 'ORGpartido', 'ORGnaogovernamental',\n",
        "                           'PESSOAcargo', 'PESSOAgrupocargo', 'PESSOAgrupoind', 'PESSOAindividual', \n",
        "                           'PRODUTOoutros', 'PRODUTOprograma', 'PRODUTOsistema']\n",
        "                      \n",
        "\n",
        "    labels_projeto_categoria = ['DATA', 'EVENTO', 'ORGANIZACAO', 'PRODUTODELEI', 'LOCAL', 'FUNDAMENTO', 'PESSOA']\n",
        "    \n",
        "    #alterar aqui\n",
        "    label_use = labels_projeto_categoria\n",
        "    \n",
        "    preds, labels = classScores(model, dataset)\n",
        "    preds = [indxToTag[item] for items in preds for item in items]\n",
        "    labels = [indxToTag[item] for items in labels for item in items]\n",
        "    model.logger.info(\"Results on {} set\".format(dataset.filename))\n",
        "    \n",
        "    #model.logger.info(classification_report(labels, preds,\n",
        "    #      labels=['JURISPRUDENCIA', 'LOCAL', 'TEMPO',  'PESSOA', 'LEGISLACAO', 'ORGANIZACAO'], digits=4))\n",
        "    #model.logger.info(f1_score(labels, preds, average='micro', labels=['JURISPRUDENCIA', 'LOCAL', 'TEMPO',  'PESSOA', 'LEGISLACAO', 'ORGANIZACAO']))\n",
        "    #SUBSTITUIR A LISTA DE LABELS ABAIXO\n",
        "    model.logger.info(classification_report(labels, preds,\n",
        "          labels=label_use, digits=4))\n",
        "    model.logger.info(f1_score(labels, preds, average='micro', labels=label_use))\n",
        "\n",
        "#if len(sys.argv) != 2 or sys.argv[1] not in [\"train\", \"test\", \"dev\"]:\n",
        "    #print(\"Usage: python classScores.py <train or test or dev>\")\n",
        "    #sys.exit(0)\n",
        "\n",
        "# create instance of config\n",
        "config = Config()\n",
        "\n",
        "#dataset = sys.argv[1]\n",
        "dataset = \"TEST\"\n",
        "if dataset == \"train\":\n",
        "    dataset = CoNLLDataset(config.filename_train, config.processing_word,\n",
        "                           config.processing_tag, config.max_iter)\n",
        "elif dataset == \"dev\":\n",
        "    dataset = CoNLLDataset(config.filename_dev, config.processing_word,\n",
        "                           config.processing_tag, config.max_iter)\n",
        "else:\n",
        "    dataset = CoNLLDataset(config.filename_test, config.processing_word,\n",
        "                           config.processing_tag, config.max_iter)\n",
        "\n",
        "main(dataset, config)\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    main(dataset, config)\n"
      ],
      "id": "PNdq_4LwCKbF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miWLKTzjJbj0"
      },
      "source": [
        "## 5.2 Evaluate\n",
        "To obtain the f1 scores (per entity) for each class on each part of the dataset:\n"
      ],
      "id": "miWLKTzjJbj0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkPIBcfIJgeM"
      },
      "outputs": [],
      "source": [
        "# This file was developed as part of the project reported in the paper below.\n",
        "# We kindly request that users cite our paper in any publication that is\n",
        "# generated as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "#from model.data_utils import CoNLLDataset\n",
        "#from model.ner_model import NERModel\n",
        "#from model.config import Config\n",
        "#import sys\n",
        "\n",
        "def align_data(data):\n",
        "    \"\"\"Given dict with lists, creates aligned strings\n",
        "\n",
        "    Adapted from Assignment 3 of CS224N\n",
        "\n",
        "    Args:\n",
        "        data: (dict) data[\"x\"] = [\"I\", \"love\", \"you\"]\n",
        "              (dict) data[\"y\"] = [\"O\", \"O\", \"O\"]\n",
        "\n",
        "    Returns:\n",
        "        data_aligned: (dict) data_align[\"x\"] = \"I love you\"\n",
        "                           data_align[\"y\"] = \"O O    O  \"\n",
        "\n",
        "    \"\"\"\n",
        "    spacings = [max([len(seq[i]) for seq in data.values()])\n",
        "                for i in range(len(data[list(data.keys())[0]]))]\n",
        "    data_aligned = dict()\n",
        "\n",
        "    # for each entry, create aligned string\n",
        "    for key, seq in data.items():\n",
        "        str_aligned = \"\"\n",
        "        for token, spacing in zip(seq, spacings):\n",
        "            str_aligned += token + \" \" * (spacing - len(token) + 1)\n",
        "\n",
        "        data_aligned[key] = str_aligned\n",
        "\n",
        "    return data_aligned\n",
        "\n",
        "\n",
        "def interactive_shell(model):\n",
        "    \"\"\"Creates interactive shell to play with model\n",
        "\n",
        "    Args:\n",
        "        model: instance of NERModel\n",
        "\n",
        "    \"\"\"\n",
        "    model.logger.info(\"\"\"\n",
        "This is an interactive mode.\n",
        "To exit, enter 'exit'.\n",
        "You can enter a sentence like\n",
        "input> I love Paris\"\"\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # for python 2\n",
        "            sentence = raw_input(\"input> \")\n",
        "        except NameError:\n",
        "            # for python 3\n",
        "            sentence = input(\"input> \")\n",
        "\n",
        "        words_raw = sentence.strip().split(\" \")\n",
        "\n",
        "        if words_raw == [\"exit\"]:\n",
        "            break\n",
        "\n",
        "        preds = model.predict(words_raw)\n",
        "        to_print = align_data({\"input\": words_raw, \"output\": preds})\n",
        "\n",
        "        for key, seq in to_print.items():\n",
        "            model.logger.info(seq)\n",
        "\n",
        "\n",
        "def main():\n",
        "        # create instance of config\n",
        "    config = Config()\n",
        "\n",
        "    # build model\n",
        "    model = NERModel(config)\n",
        "    model.build()\n",
        "    model.restore_session(config.dir_model)\n",
        "\n",
        "    # create dataset\n",
        "    #dataset = sys.argv[1]\n",
        "    dataset = \"test\"\n",
        "    if dataset == \"train\":\n",
        "        dataset = CoNLLDataset(config.filename_train, config.processing_word,\n",
        "                            config.processing_tag, config.max_iter)\n",
        "    elif dataset == \"dev\":\n",
        "        dataset = CoNLLDataset(config.filename_dev, config.processing_word,\n",
        "                            config.processing_tag, config.max_iter)\n",
        "    else:\n",
        "        dataset = CoNLLDataset(config.filename_test, config.processing_word,\n",
        "                            config.processing_tag, config.max_iter)\n",
        "\n",
        "    # evaluate and interact\n",
        "    model.evaluate(dataset)\n",
        "    interactive_shell(model)\n",
        "\n",
        "\n",
        "#if len(sys.argv) != 2 or sys.argv[1] not in [\"train\", \"test\", \"dev\"]:\n",
        "        #print(\"Usage: python evaluate.py <train or test or dev>\")\n",
        "        #sys.exit(0)\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        " #   main()\n",
        "print(\"rodada: \", DIR)\n",
        "print()\n",
        "main()\n"
      ],
      "id": "gkPIBcfIJgeM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLoZSpzDLlj6"
      },
      "source": [
        "## 5.3 EvaluateText\n",
        "To tag a raw text file\n"
      ],
      "id": "sLoZSpzDLlj6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W43e9IUeLe2Z"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code entirely written by the authors of leNER-Br paper\n",
        "'''\n",
        "\n",
        "# This file was developed as part of the project reported in the paper below.\n",
        "# We kindly request that users cite our paper in any publication that is \n",
        "# generated as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "\n",
        "#from model.ner_model import NERModel\n",
        "#from model.config import Config\n",
        "from nltk import word_tokenize\n",
        "from nltk import data\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "#import sys\n",
        "\n",
        "bcolors_lener = {\n",
        "    \"PESSOA\": '\\033[94m',\n",
        "    \"TEMPO\": '\\033[92m',\n",
        "    \"LOCAL\": '\\033[93m',\n",
        "    \"ORGANIZACAO\": '\\033[91m',\n",
        "    \"JURISPRUDENCIA\": '\\033[35m',\n",
        "    \"LEGISLACAO\": '\\033[36m',\n",
        "    \"ENDC\": '\\033[0m',\n",
        "    \"O\": \"\"\n",
        "}\n",
        "\n",
        "labels_projeto_tipo = {\n",
        "    'DATA': '\\033[92m', \n",
        "    'EVENTO': '\\033[95m',\n",
        "    'FUNDapelido': '\\033[36m',\n",
        "    'FUNDlei': '\\033[36m',\n",
        "    'FUNDprojetodelei': '\\033[36m',\n",
        "    'FUNDsolicitacaotrabalho': '\\033[36m',\n",
        "    'LOCALconcreto': '\\033[93m',\n",
        "    'LOCALvirtual': '\\033[93m',\n",
        "    'ORGgovernamental': '\\033[91m',\n",
        "    'ORGpartido': '\\033[91m',\n",
        "    'ORGnaogovernamental': '\\033[91m',\n",
        "    'PESSOAcargo': '\\033[94m',\n",
        "    'PESSOAgrupocargo': '\\033[94m',\n",
        "    'PESSOAgrupoind': '\\033[94m',\n",
        "    'PESSOAindividual': '\\033[94m',\n",
        "    'PRODUTOoutros': '\\033[96m',\n",
        "    'PRODUTOprograma': '\\033[96m',\n",
        "    'PRODUTOsistema': '\\033[96m',\n",
        "    \"ENDC\": '\\033[0m',\n",
        "    \"O\": \"\"\n",
        "}\n",
        "\n",
        "labels_projeto_categoria = {\n",
        "    'DATA': '\\033[92m', #verde\n",
        "    'EVENTO': '\\033[95m', #magenta\n",
        "    'ORGANIZACAO': '\\033[91m', #vermelho\n",
        "    'PRODUTODELEI': '\\033[96m', #ciano\n",
        "    'LOCAL': '\\033[93m', #amarelo\n",
        "    'FUNDAMENTO': '\\033[36m', #ciano\n",
        "    'PESSOA': '\\033[94m', #azul\n",
        "    \"ENDC\": '\\033[0m',\n",
        "    \"O\": \"\"\n",
        "}\n",
        "\n",
        "\n",
        "#berg alterar as cores\n",
        "bcolors = labels_projeto_tipo\n",
        "\n",
        "# create instance of config\n",
        "config = Config()\n",
        "\n",
        "# build model\n",
        "model = NERModel(config)\n",
        "model.build()\n",
        "model.restore_session(config.dir_model)\n",
        "\n",
        "filename = \"/content/texto_pra_teste_corrigido.txt\"\n",
        "\n",
        "tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "with open(filename, 'r', encoding = \"UTF-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokenizer.train(text)\n",
        "sentences = tokenizer.tokenize(text)\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence, language='portuguese')\n",
        "    preds = model.predict(words)\n",
        "    for index, word in enumerate(words):\n",
        "        if preds[index][0:2] in ['B-', 'I-', 'E-', 'S-']:\n",
        "            preds[index] = preds[index][2:]\n",
        "        print(bcolors[preds[index]] +\n",
        "              word + bcolors[\"ENDC\"], end=' ')\n",
        "    print('\\n')"
      ],
      "id": "W43e9IUeLe2Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fkSCGTBpI9"
      },
      "source": [
        "## 5.4 EvaluateSentence\n",
        "To tag sentences in a interactive way:"
      ],
      "id": "B_fkSCGTBpI9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR4rlN6COhtm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code entirely written by the leNER-Br paper authors\n",
        "'''\n",
        "\n",
        "# This file was developed as part of the project reported in the paper below.\n",
        "# We kindly request that users cite our paper in any publication that is \n",
        "# generated as a result of the use of our source code or our dataset.\n",
        "# \n",
        "# Pedro H. Luz de Araujo, Teófilo E. de Campos, Renato R. R. de Oliveira, Matheus Stauffer, Samuel Couto and Paulo Bermejo.\n",
        "# LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text.\n",
        "# International Conference on the Computational Processing of Portuguese (PROPOR),\n",
        "# September 24-26, Canela, Brazil, 2018. \n",
        "#\n",
        "#    @InProceedings{luz_etal_propor2018,\n",
        "#          author = {Pedro H. {Luz de Araujo} and Te\\'{o}filo E. {de Campos} and\n",
        "#          Renato R. R. {de Oliveira} and Matheus Stauffer and\n",
        "#          Samuel Couto and Paulo Bermejo},\n",
        "#          title = {LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text},\n",
        "#          booktitle = {International Conference on the Computational Processing of Portuguese\n",
        "#          ({PROPOR})},\n",
        "#          year = {2018},\n",
        "#          month = {September 24-26},\n",
        "#          address = {Canela, RS, Brazil},\n",
        "#          note = {Available from \\url{https://cic.unb.br/~teodecampos/LeNER-Br/}}\n",
        "#      }      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from model.ner_model import NERModel\n",
        "#from model.config import Config\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Pessoa is blue, tempo is green, Local is yellow and organizacao is red\n",
        "bcolors = {\n",
        "    \"PESSOA\": '\\033[94m',\n",
        "    \"TEMPO\": '\\033[92m',\n",
        "    \"LOCAL\": '\\033[93m',\n",
        "    \"ORGANIZACAO\": '\\033[91m',\n",
        "    \"JURISPRUDENCIA\": '\\033[35m',\n",
        "    \"LEGISLACAO\": '\\033[36m',\n",
        "    \"ENDC\": '\\033[0m',\n",
        "    \"O\": \"\"\n",
        "}\n",
        "\n",
        "# create instance of config\n",
        "config = Config()\n",
        "\n",
        "# build model\n",
        "model = NERModel(config)\n",
        "model.build()\n",
        "model.restore_session(config.dir_model)\n",
        "\n",
        "while(True):\n",
        "    words = input(\"Escreva frase a ser analisada: \")\n",
        "    words = word_tokenize(words, language='portuguese')\n",
        "    preds = model.predict(words)\n",
        "    for index, word in enumerate(words):\n",
        "        if preds[index][0:2] in ['B-', 'I-', 'E-', 'S-']:\n",
        "            preds[index] = preds[index][2:]\n",
        "        print(bcolors[preds[index]] +\n",
        "              word + bcolors[\"ENDC\"], end=' ')\n",
        "    print('\\n')"
      ],
      "id": "WR4rlN6COhtm"
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "collapsed_sections": [
        "bPONohvQCbwZ",
        "fd9eNV1oAo_S",
        "WQhrN-IFA3Ja",
        "8nbIE-bsBCo1",
        "6KB0hNsmBYGP",
        "qDw9i-vfBmJe",
        "2WelIy4K2sPW",
        "LUOkBCRxB0VO",
        "lrrQ-nsVJVDW",
        "miWLKTzjJbj0",
        "sLoZSpzDLlj6",
        "B_fkSCGTBpI9"
      ],
      "name": "lener-br-master_original.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}