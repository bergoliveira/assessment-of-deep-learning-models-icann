{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adaptado de:\n",
        "https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb"
      ],
      "metadata": {
        "id": "A6x3z9I4DNuq"
      },
      "id": "A6x3z9I4DNuq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71ad942b"
      },
      "source": [
        "# 1. Configs\n"
      ],
      "id": "71ad942b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFVqLNSBN5fS",
        "outputId": "2774ed71-9451-4bb5-c4f0-37aed95e6ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 20 21:22:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#teste gpu\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "rFVqLNSBN5fS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6RkbV2fqC3d",
        "outputId": "9d3339bb-39ea-4643-ff60-21ed6cebd0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#teste ram\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "C6RkbV2fqC3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8vb_VzndAf6",
        "outputId": "a133facd-6970-458e-85cb-b42a4798ff79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#drive e variÃ¡veis globais\n",
        "#utilizar GPU\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "_DATASET_TO_USE = 'pl_corpus' #lener_br"
      ],
      "id": "J8vb_VzndAf6"
    },
    {
      "cell_type": "code",
      "source": [
        "#installs\n",
        "%%capture\n",
        "!pip install datasets transformers seqeval\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "OMdwLRt7D3O9"
      },
      "id": "OMdwLRt7D3O9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b103112",
        "outputId": "965c74a6-3d9b-43af-e445-579c56d71078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.16.2\n",
            "1.18.3\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "print(transformers.__version__) \n",
        "print(datasets.__version__) "
      ],
      "id": "2b103112"
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT pre-treinado Lener\n",
        "model_checkpoint = \"pierreguillou/bert-base-cased-pt-lenerbr\"\n",
        "\n",
        "#dtaset para fine-tunning\n",
        "if _DATASET_TO_USE == 'lener_br':\n",
        "  datasets = load_dataset(\"lener_br\")\n",
        "else:\n",
        "  path_plcorpus_dataset = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/pl_corpus_dataset/'  \n",
        "  url_files = 'https://raw.githubusercontent.com/bergoliveira/disciplinaDL/main/pl_corpus/'\n",
        "  datasets = load_dataset('/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/pl_corpus_dataset/pl_corpus.py', \n",
        "                        data_files={'train': [url_files+'train.conll', url_files+'dev.conll'], 'test': url_files+'test.conll'})\n",
        "#tarefa\n",
        "task = \"ner\" \n",
        "\n",
        "#entidades no modelo IOB\n",
        "#datasets[\"train\"].features[f\"ner_tags\"]\n",
        "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "f59201d9112545c3aafd57835dcb9fdb",
            "13c0dd134cb14c4badbc541d1a0c16c3",
            "a63763e2f0e3408ca8680e4f5e6f063e",
            "e9e3db83f2a8451b8623c5b7e55d96ea",
            "9b8157e4d62d40d0ac636991429a1bd0",
            "8390a1253fd64635bf4c5f13e62196b3",
            "f1f7f09c8b2a46caaebe8619179a39d5",
            "38862b4461b34d9ca1065bd9fe42f3b0",
            "beba4fe7ec7c4f4f99769c7de42db620",
            "a1283e329a164a5780f705e912c4bbdd",
            "452ac762364d4dde93beb561fde15f9e"
          ]
        },
        "id": "SN_43pptFlXI",
        "outputId": "786d4ca1-af76-4536-8e53-dbda792d83ac"
      },
      "id": "SN_43pptFlXI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-7c613bab45c2950d\n",
            "Reusing dataset pl_corpus (/root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f59201d9112545c3aafd57835dcb9fdb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-ORGANIZACAO',\n",
              " 'I-ORGANIZACAO',\n",
              " 'B-PESSOA',\n",
              " 'I-PESSOA',\n",
              " 'B-DATA',\n",
              " 'I-DATA',\n",
              " 'B-LOCAL',\n",
              " 'I-LOCAL',\n",
              " 'B-FUNDAMENTO',\n",
              " 'I-FUNDAMENTO',\n",
              " 'B-PRODUTODELEI',\n",
              " 'I-PRODUTODELEI',\n",
              " 'B-EVENTO',\n",
              " 'I-EVENTO']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrar elementos do dataset\n",
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "show_random_elements(datasets[\"train\"])"
      ],
      "metadata": {
        "id": "IiWmEYzOGK-V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "cb1ef01b-00a5-4e06-b262-5990cd7085c6"
      },
      "id": "IiWmEYzOGK-V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2732</td>\n",
              "      <td>[II, â, em, caso, de, empate, na, votaÃ§Ã£o, ,, a, candidata, mulher, ,, observando-se, a, ordem, decrescente, de, idade, se, ocorrer, empate, entre, candidatas, mulheres, ou, entre, candidatos, homens, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3407</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>789</td>\n",
              "      <td>[Cabe, destacar, ,, mais, uma, vez, ,, que, ,, segundo, a, legislaÃ§Ã£o, previdenciÃ¡ria, vigente, ,, este, adicional, sÃ³, Ã©, concedido, aos, aposentados, por, invalidez, ,, nÃ£o, sendo, estendido, a, segurados, do, RGPS, que, tenham, se, aposentado, por, idade, ou, por, tempo, de, contribuiÃ§Ã£o, e, que, tambÃ©m, necessitem, do, auxÃ­lio, permanente, de, terceiros, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PRODUTODELEI, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4568</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2813</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2306</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4219</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>635</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2082</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>437</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-processing"
      ],
      "metadata": {
        "id": "w3T8DSfDje2G"
      },
      "id": "w3T8DSfDje2G"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "label_all_tokens = True\n",
        "\n",
        "#example\n",
        "example = datasets[\"train\"][5]\n",
        "print(example[\"tokens\"])\n",
        "\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "\n",
        "word_ids = tokenized_input.word_ids()\n",
        "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
        "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "Nf8BgZ7yGhnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382ffaf2-8c29-49c3-f4ca-3c9b978fbe80"
      },
      "id": "Nf8BgZ7yGhnN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Â§', '1Âº', 'O', 'regulamento', 'disporÃ¡', 'sobre', 'a', 'definiÃ§Ã£o', 'das', 'dimensÃµes', 'dos', 'calÃ§ados', 'que', 'terÃ£o', 'sua', 'comercializaÃ§Ã£o', 'vedada', ',', 'nos', 'termos', 'do', 'caput', 'deste', 'artigo']\n",
            "['[CLS]', 'Â§', '[UNK]', 'O', 'regulamento', 'disp', '##or', '##Ã¡', 'sobre', 'a', 'definiÃ§Ã£o', 'das', 'dimensÃµes', 'dos', 'cal', '##Ã§ados', 'que', 'terÃ£o', 'sua', 'comercializaÃ§Ã£o', 've', '##dad', '##a', ',', 'nos', 'termos', 'do', 'cap', '##ut', 'deste', 'artigo', '[SEP]']\n",
            "32 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizaÃ§Ã£o\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenize_and_align_labels(datasets['train'][:5])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "WtjMdDwCHRnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5439d63-65b9-4082-80c8-98ecbb46ef7a"
      },
      "id": "WtjMdDwCHRnp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-785385adef2b6b4c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-3a8ffc9f1bddbe0b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-8550f6e31db87d82.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Finetunning"
      ],
      "metadata": {
        "id": "YRm2vEO-kucx"
      },
      "id": "YRm2vEO-kucx"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svqRhn66Hfd_",
        "outputId": "3afe6c36-3200-483e-c920-ed2c0abb058a"
      },
      "id": "svqRhn66Hfd_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pierreguillou/bert-base-cased-pt-lenerbr were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at pierreguillou/bert-base-cased-pt-lenerbr and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Treinamento"
      ],
      "metadata": {
        "id": "aF8h5NMO7y7P"
      },
      "id": "aF8h5NMO7y7P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 ConnfiguraÃ§Ãµes do modelo"
      ],
      "metadata": {
        "id": "OXGxtPNK7_LC"
      },
      "id": "OXGxtPNK7_LC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "# hyperparameters utilizado no teste\n",
        "per_device_batch_size = 5  #[5, 10, 20] cada batch por thread, duplicando os valores.\n",
        "num_train_epochs = 55 # [35, 55, 75] pierre: we started with 10 epochs but the model overfits fastly\n",
        "learning_rate = 2e-5 #[1e-4,2e-5,3e-4,5e-5] pierre: (AdamW) we started with 3e-4, then 1e-4, then 5e-5 but the model overfits fastly\n",
        "\n",
        "# hyperparameters nÃ£o modificados\n",
        "gradient_accumulation_steps = 2\n",
        "weight_decay = 0.01\n",
        "save_total_limit = 100 ##early stopping\n",
        "logging_steps = 290 #melhor evaluate frequently (5000 seems too high)\n",
        "eval_steps = logging_steps\n",
        "evaluation_strategy = 'epoch' #pierre; 'steps'\n",
        "logging_strategy = 'epoch' #'pierre; 'steps'\n",
        "save_strategy = 'epoch' #pierre; 'steps'\n",
        "save_steps = logging_steps\n",
        "load_best_model_at_end = True\n",
        "\n",
        "fp16 = True\n",
        "\n",
        "# folders\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "folder_model = 'e' + str(num_train_epochs) + '_lr' + str(learning_rate)\n",
        "if _DATASET_TO_USE == 'lener_br':\n",
        "  output_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/' + 'ner-lenerbr-' + str(model_name) + '/checkpoints/' + folder_model\n",
        "  logging_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/logs/' + 'ner-lenerbr-' + str(model_name) + '/logs/' + folder_model\n",
        "else:\n",
        "  output_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/' + 'ner-pl_corpus-' + str(model_name) + '/checkpoints/' + folder_model\n",
        "  logging_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/logs/' + 'ner-pl_corpus-' + str(model_name) + '/logs/' + folder_model\n",
        "\n",
        "# metrics\n",
        "metric_for_best_model = 'eval_f1'\n",
        "if metric_for_best_model == 'eval_f1':\n",
        "    greater_is_better = True\n",
        "elif metric_for_best_model == 'eval_loss':\n",
        "    greater_is_better = False  \n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_batch_size, \n",
        "    per_device_eval_batch_size=per_device_batch_size*2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_train_epochs, \n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=save_total_limit,\n",
        "    logging_steps = logging_steps,\n",
        "    eval_steps = logging_steps,\n",
        "    load_best_model_at_end = load_best_model_at_end,\n",
        "    metric_for_best_model = metric_for_best_model,\n",
        "    greater_is_better = greater_is_better,\n",
        "    gradient_checkpointing = False,\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    do_predict = True,\n",
        "    evaluation_strategy = evaluation_strategy,\n",
        "    logging_dir=logging_dir, \n",
        "    logging_strategy = logging_strategy,\n",
        "    save_strategy = save_strategy,\n",
        "    save_steps = save_steps,\n",
        "    fp16 = fp16,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "wEH_Ppd-HmfV"
      },
      "id": "wEH_Ppd-HmfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "#labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
        "#metric.compute(predictions=[labels], references=[labels])"
      ],
      "metadata": {
        "id": "O81TuJSZMEMX"
      },
      "id": "O81TuJSZMEMX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "gfYSuUlJMQAL"
      },
      "id": "gfYSuUlJMQAL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 ExecuÃ§Ã£o"
      ],
      "metadata": {
        "id": "swVzpkGFqCVe"
      },
      "id": "swVzpkGFqCVe"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# wait early_stopping_patience x eval_steps before to stp the training in order to get a better model\n",
        "early_stopping_patience = save_total_limit\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "t-zD9ka_MVIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "fc8a6c5b-d186-4d6f-8a15-4240405ae0ec"
      },
      "id": "t-zD9ka_MVIo",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, tokens.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 4770\n",
            "  Num Epochs = 55\n",
            "  Instantaneous batch size per device = 5\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 26235\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='574' max='26235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  574/26235 01:27 < 1:05:36, 6.52 it/s, Epoch 1.20/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.023100</td>\n",
              "      <td>0.058047</td>\n",
              "      <td>0.786949</td>\n",
              "      <td>0.810920</td>\n",
              "      <td>0.798755</td>\n",
              "      <td>0.969764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='719' max='26235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  719/26235 01:47 < 1:03:47, 6.67 it/s, Epoch 1.51/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.023100</td>\n",
              "      <td>0.058047</td>\n",
              "      <td>0.786949</td>\n",
              "      <td>0.810920</td>\n",
              "      <td>0.798755</td>\n",
              "      <td>0.969764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9b9975767deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 ):\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. AvaliaÃ§Ã£o"
      ],
      "metadata": {
        "id": "o_P5XtsuqEGP"
      },
      "id": "o_P5XtsuqEGP"
    },
    {
      "cell_type": "code",
      "source": [
        "#geral\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "7WDSGK8COxnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "5ec69d00-7fe2-457e-ab9b-b9e021add36f"
      },
      "id": "7WDSGK8COxnj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1177\n",
            "  Batch size = 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='783' max='21505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  783/21505 04:32 < 2:00:36, 2.86 it/s, Epoch 2.00/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.217200</td>\n",
              "      <td>0.127338</td>\n",
              "      <td>0.807813</td>\n",
              "      <td>0.831613</td>\n",
              "      <td>0.819540</td>\n",
              "      <td>0.961677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.094852</td>\n",
              "      <td>0.834428</td>\n",
              "      <td>0.901720</td>\n",
              "      <td>0.866770</td>\n",
              "      <td>0.973319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.094852</td>\n",
              "      <td>0.834428</td>\n",
              "      <td>0.901720</td>\n",
              "      <td>0.866770</td>\n",
              "      <td>0.973319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.9733189083761257,\n",
              " 'eval_f1': 0.8667700258397932,\n",
              " 'eval_loss': 0.09485199302434921,\n",
              " 'eval_precision': 0.8344278606965174,\n",
              " 'eval_recall': 0.9017204301075269}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prediÃ§Ã£o por entidades\n",
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "CkDz4prVO6Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "3f4f59c4-e198-4c9a-fa1a-a26ae8645d8b"
      },
      "id": "CkDz4prVO6Ed",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, tokens.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1177\n",
            "  Batch size = 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='783' max='21505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  783/21505 04:32 < 2:00:36, 2.86 it/s, Epoch 2.00/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.217200</td>\n",
              "      <td>0.127338</td>\n",
              "      <td>0.807813</td>\n",
              "      <td>0.831613</td>\n",
              "      <td>0.819540</td>\n",
              "      <td>0.961677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.094852</td>\n",
              "      <td>0.834428</td>\n",
              "      <td>0.901720</td>\n",
              "      <td>0.866770</td>\n",
              "      <td>0.973319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.047200</td>\n",
              "      <td>0.094852</td>\n",
              "      <td>0.834428</td>\n",
              "      <td>0.901720</td>\n",
              "      <td>0.866770</td>\n",
              "      <td>0.973319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'JURISPRUDENCIA': {'f1': 0.6703755215577191,\n",
              "  'number': 657,\n",
              "  'precision': 0.617157490396927,\n",
              "  'recall': 0.7336377473363774},\n",
              " 'LEGISLACAO': {'f1': 0.8468013468013469,\n",
              "  'number': 571,\n",
              "  'precision': 0.8152350081037277,\n",
              "  'recall': 0.8809106830122592},\n",
              " 'LOCAL': {'f1': 0.759493670886076,\n",
              "  'number': 194,\n",
              "  'precision': 0.746268656716418,\n",
              "  'recall': 0.7731958762886598},\n",
              " 'ORGANIZACAO': {'f1': 0.8480392156862745,\n",
              "  'number': 1340,\n",
              "  'precision': 0.7988126649076517,\n",
              "  'recall': 0.9037313432835821},\n",
              " 'PESSOA': {'f1': 0.9868421052631579,\n",
              "  'number': 1072,\n",
              "  'precision': 0.9943181818181818,\n",
              "  'recall': 0.9794776119402985},\n",
              " 'TEMPO': {'f1': 0.9544910179640718,\n",
              "  'number': 816,\n",
              "  'precision': 0.9332552693208431,\n",
              "  'recall': 0.9767156862745098},\n",
              " 'overall_accuracy': 0.9733189083761257,\n",
              " 'overall_f1': 0.8667700258397932,\n",
              " 'overall_precision': 0.8344278606965174,\n",
              " 'overall_recall': 0.9017204301075269}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "collapsed_sections": [
        "o_P5XtsuqEGP"
      ],
      "name": "projetoDL_Bert.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f59201d9112545c3aafd57835dcb9fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13c0dd134cb14c4badbc541d1a0c16c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a63763e2f0e3408ca8680e4f5e6f063e",
              "IPY_MODEL_e9e3db83f2a8451b8623c5b7e55d96ea",
              "IPY_MODEL_9b8157e4d62d40d0ac636991429a1bd0"
            ]
          }
        },
        "13c0dd134cb14c4badbc541d1a0c16c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a63763e2f0e3408ca8680e4f5e6f063e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8390a1253fd64635bf4c5f13e62196b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1f7f09c8b2a46caaebe8619179a39d5"
          }
        },
        "e9e3db83f2a8451b8623c5b7e55d96ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_38862b4461b34d9ca1065bd9fe42f3b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_beba4fe7ec7c4f4f99769c7de42db620"
          }
        },
        "9b8157e4d62d40d0ac636991429a1bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1283e329a164a5780f705e912c4bbdd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 92.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_452ac762364d4dde93beb561fde15f9e"
          }
        },
        "8390a1253fd64635bf4c5f13e62196b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1f7f09c8b2a46caaebe8619179a39d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38862b4461b34d9ca1065bd9fe42f3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "beba4fe7ec7c4f4f99769c7de42db620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1283e329a164a5780f705e912c4bbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "452ac762364d4dde93beb561fde15f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}