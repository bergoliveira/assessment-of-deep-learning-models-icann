{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adaptado de:\n",
        "https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb"
      ],
      "metadata": {
        "id": "A6x3z9I4DNuq"
      },
      "id": "A6x3z9I4DNuq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71ad942b"
      },
      "source": [
        "# 1. Configs\n"
      ],
      "id": "71ad942b"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFVqLNSBN5fS",
        "outputId": "194da3b1-20c7-47d1-9544-86dcb18d1db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 20 21:37:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#teste gpu\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "rFVqLNSBN5fS"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6RkbV2fqC3d",
        "outputId": "d46882aa-cdec-4d68-c743-1456dd69f7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#teste ram\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "C6RkbV2fqC3d"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8vb_VzndAf6",
        "outputId": "e1236773-55af-4803-ea7b-8a8a931367ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#drive e variáveis globais\n",
        "#utilizar GPU\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "_DATASET_TO_USE = 'pl_corpus' #lener_br"
      ],
      "id": "J8vb_VzndAf6"
    },
    {
      "cell_type": "code",
      "source": [
        "#installs\n",
        "%%capture\n",
        "!pip install datasets transformers seqeval\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "OMdwLRt7D3O9"
      },
      "id": "OMdwLRt7D3O9",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b103112",
        "outputId": "52c9857b-519a-4256-fa2c-772f13c39f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.16.2\n",
            "1.18.3\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "print(transformers.__version__) \n",
        "print(datasets.__version__) "
      ],
      "id": "2b103112"
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT pre-treinado Lener\n",
        "model_checkpoint = \"pierreguillou/bert-base-cased-pt-lenerbr\"\n",
        "\n",
        "#dtaset para fine-tunning\n",
        "if _DATASET_TO_USE == 'lener_br':\n",
        "  datasets = load_dataset(\"lener_br\")\n",
        "else:\n",
        "  path_plcorpus_dataset = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/pl_corpus_dataset/'  \n",
        "  url_files = 'https://raw.githubusercontent.com/bergoliveira/disciplinaDL/main/pl_corpus/'\n",
        "  datasets = load_dataset('/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/pl_corpus_dataset/pl_corpus.py', \n",
        "                        data_files={'train': [url_files+'train.conll', url_files+'dev.conll'], 'test': url_files+'test.conll'})\n",
        "#tarefa\n",
        "task = \"ner\" \n",
        "\n",
        "#entidades no modelo IOB\n",
        "#datasets[\"train\"].features[f\"ner_tags\"]\n",
        "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
        "label_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "713bc89c7b204021a5cf5ad67e8c86ec",
            "24fc312db5dc4a5987986fc624c380d0",
            "99213bdb689a44a7889b52377c38e321",
            "d0b41ae478044aa2bc596372cc7b34e6",
            "088ebcdd83674bf283bff0eb2f59b9b8",
            "59a2ea7c3f6446d98a7c6bf581055c52",
            "0d017659129a40588651a1a9725ebad4",
            "0a3f4040dd834d1ab845b88b532e0809",
            "e672015cb3164fa289ba5db650c9424c",
            "18b2013cca3c4705b124ed5e3a44504d",
            "4d51bb0f4bc7481c9f5ff5f5a3ebdc25"
          ]
        },
        "id": "SN_43pptFlXI",
        "outputId": "9b8af0fd-959f-402d-eebd-732d389631bb"
      },
      "id": "SN_43pptFlXI",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-7c613bab45c2950d\n",
            "Reusing dataset pl_corpus (/root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713bc89c7b204021a5cf5ad67e8c86ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-ORGANIZACAO',\n",
              " 'I-ORGANIZACAO',\n",
              " 'B-PESSOA',\n",
              " 'I-PESSOA',\n",
              " 'B-DATA',\n",
              " 'I-DATA',\n",
              " 'B-LOCAL',\n",
              " 'I-LOCAL',\n",
              " 'B-FUNDAMENTO',\n",
              " 'I-FUNDAMENTO',\n",
              " 'B-PRODUTODELEI',\n",
              " 'I-PRODUTODELEI',\n",
              " 'B-EVENTO',\n",
              " 'I-EVENTO']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mostrar elementos do dataset\n",
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "show_random_elements(datasets[\"train\"])"
      ],
      "metadata": {
        "id": "IiWmEYzOGK-V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d91c8a87-7c64-424d-ef74-0acc31dd6a0e"
      },
      "id": "IiWmEYzOGK-V",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tokens</th>\n",
              "      <th>ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2452</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3818</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4144</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2894</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1443</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2480</td>\n",
              "      <td>[Deputado, HEITOR, FREIRE, 2, Reportagem, veiculada, pelo, Valor, Econômico, .]</td>\n",
              "      <td>[B-PESSOA, B-PESSOA, I-PESSOA, O, O, O, O, B-LOCAL, I-LOCAL, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1498</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3032</td>\n",
              "      <td>[Nesse, sentido, ,, esperamos, que, a, adoção, das, medidas, propostas, dê, maior, liberdade, para, que, os, agentes, econômicos, fortalecem, seus, empreendimentos, ,, o, que, contribuiria, para, aumentar, os, investimentos, e, o, consumo, das, famílias, e, ,, consequentemente, ,, reduziria, os, níveis, de, desemprego, ,, razões, pelas, quais, contamos, com, o, apoio, dos, nobres, colegas, Parlamentares, para, o, aperfeiçoamento, e, a, aprovação, do, presente, Projeto, de, Lei, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PESSOA, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>285</td>\n",
              "      <td>[Sala, das, Sessões, ,, em, de, março, de, 2011, .]</td>\n",
              "      <td>[O, O, O, O, O, O, B-DATA, I-DATA, I-DATA, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1945</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-processing"
      ],
      "metadata": {
        "id": "w3T8DSfDje2G"
      },
      "id": "w3T8DSfDje2G"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "label_all_tokens = True\n",
        "\n",
        "#example\n",
        "example = datasets[\"train\"][5]\n",
        "print(example[\"tokens\"])\n",
        "\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "\n",
        "word_ids = tokenized_input.word_ids()\n",
        "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
        "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "Nf8BgZ7yGhnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c196f4b3-552b-4161-ccf8-7a2713c71b2f"
      },
      "id": "Nf8BgZ7yGhnN",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['§', '1º', 'O', 'regulamento', 'disporá', 'sobre', 'a', 'definição', 'das', 'dimensões', 'dos', 'calçados', 'que', 'terão', 'sua', 'comercialização', 'vedada', ',', 'nos', 'termos', 'do', 'caput', 'deste', 'artigo']\n",
            "['[CLS]', '§', '[UNK]', 'O', 'regulamento', 'disp', '##or', '##á', 'sobre', 'a', 'definição', 'das', 'dimensões', 'dos', 'cal', '##çados', 'que', 'terão', 'sua', 'comercialização', 've', '##dad', '##a', ',', 'nos', 'termos', 'do', 'cap', '##ut', 'deste', 'artigo', '[SEP]']\n",
            "32 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenização\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenize_and_align_labels(datasets['train'][:5])\n",
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "WtjMdDwCHRnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bcab8f1-8d7a-4c24-9f2c-096aba5f891c"
      },
      "id": "WtjMdDwCHRnp",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-785385adef2b6b4c.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-3a8ffc9f1bddbe0b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/pl_corpus/default-7c613bab45c2950d/1.0.0/711af93da4644ff8dff30ef9308e16d45352ea7e9e58c9c40391900cfb3f06c5/cache-8550f6e31db87d82.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Finetunning"
      ],
      "metadata": {
        "id": "YRm2vEO-kucx"
      },
      "id": "YRm2vEO-kucx"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svqRhn66Hfd_",
        "outputId": "16a6c526-ba49-4859-90c3-87f779eb6cfb"
      },
      "id": "svqRhn66Hfd_",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pierreguillou/bert-base-cased-pt-lenerbr were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at pierreguillou/bert-base-cased-pt-lenerbr and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Treinamento"
      ],
      "metadata": {
        "id": "aF8h5NMO7y7P"
      },
      "id": "aF8h5NMO7y7P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Connfigurações do modelo"
      ],
      "metadata": {
        "id": "OXGxtPNK7_LC"
      },
      "id": "OXGxtPNK7_LC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "# hyperparameters utilizado no teste\n",
        "per_device_batch_size = 5  #[5, 10, 20] cada batch por thread, duplicando os valores.\n",
        "num_train_epochs = 55 # [35, 55, 75] pierre: we started with 10 epochs but the model overfits fastly\n",
        "learning_rate = 2e-5 #[1e-4,2e-5,3e-4,5e-5] pierre: (AdamW) we started with 3e-4, then 1e-4, then 5e-5 but the model overfits fastly\n",
        "\n",
        "# hyperparameters não modificados\n",
        "gradient_accumulation_steps = 2\n",
        "weight_decay = 0.01\n",
        "save_total_limit = 100 ##early stopping\n",
        "logging_steps = 290 #melhor evaluate frequently (5000 seems too high)\n",
        "eval_steps = logging_steps\n",
        "evaluation_strategy = 'epoch' #pierre; 'steps'\n",
        "logging_strategy = 'epoch' #'pierre; 'steps'\n",
        "save_strategy = 'epoch' #pierre; 'steps'\n",
        "save_steps = logging_steps\n",
        "load_best_model_at_end = True\n",
        "\n",
        "fp16 = True\n",
        "\n",
        "# folders\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "folder_model = 'e' + str(num_train_epochs) + '_lr' + str(learning_rate)\n",
        "if _DATASET_TO_USE == 'lener_br':\n",
        "  output_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/' + 'ner-lenerbr-' + str(model_name) + '/checkpoints/' + folder_model\n",
        "  logging_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/logs/' + 'ner-lenerbr-' + str(model_name) + '/logs/' + folder_model\n",
        "else:\n",
        "  output_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/' + 'ner-pl_corpus-' + str(model_name) + '/checkpoints/' + folder_model\n",
        "  logging_dir = '/content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/logs/' + 'ner-pl_corpus-' + str(model_name) + '/logs/' + folder_model\n",
        "\n",
        "# metrics\n",
        "metric_for_best_model = 'eval_f1'\n",
        "if metric_for_best_model == 'eval_f1':\n",
        "    greater_is_better = True\n",
        "elif metric_for_best_model == 'eval_loss':\n",
        "    greater_is_better = False  \n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=per_device_batch_size, \n",
        "    per_device_eval_batch_size=per_device_batch_size*2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_train_epochs, \n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=save_total_limit,\n",
        "    logging_steps = logging_steps,\n",
        "    eval_steps = logging_steps,\n",
        "    load_best_model_at_end = load_best_model_at_end,\n",
        "    metric_for_best_model = metric_for_best_model,\n",
        "    greater_is_better = greater_is_better,\n",
        "    gradient_checkpointing = False,\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    do_predict = True,\n",
        "    evaluation_strategy = evaluation_strategy,\n",
        "    logging_dir=logging_dir, \n",
        "    logging_strategy = logging_strategy,\n",
        "    save_strategy = save_strategy,\n",
        "    save_steps = save_steps,\n",
        "    fp16 = fp16,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "wEH_Ppd-HmfV"
      },
      "id": "wEH_Ppd-HmfV",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "#labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
        "#metric.compute(predictions=[labels], references=[labels])"
      ],
      "metadata": {
        "id": "O81TuJSZMEMX"
      },
      "id": "O81TuJSZMEMX",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "gfYSuUlJMQAL"
      },
      "id": "gfYSuUlJMQAL",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Execução"
      ],
      "metadata": {
        "id": "swVzpkGFqCVe"
      },
      "id": "swVzpkGFqCVe"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# wait early_stopping_patience x eval_steps before to stp the training in order to get a better model\n",
        "early_stopping_patience = save_total_limit\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "t-zD9ka_MVIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93bfd053-a0e8-4da0-aa69-eeff4f324052"
      },
      "id": "t-zD9ka_MVIo",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using amp half precision backend\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 4770\n",
            "  Num Epochs = 55\n",
            "  Instantaneous batch size per device = 5\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 26235\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26235' max='26235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26235/26235 1:09:53, Epoch 55/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.217800</td>\n",
              "      <td>0.048687</td>\n",
              "      <td>0.669430</td>\n",
              "      <td>0.742529</td>\n",
              "      <td>0.704087</td>\n",
              "      <td>0.967723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>0.051966</td>\n",
              "      <td>0.770764</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.785110</td>\n",
              "      <td>0.969318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.021200</td>\n",
              "      <td>0.056218</td>\n",
              "      <td>0.725451</td>\n",
              "      <td>0.832184</td>\n",
              "      <td>0.775161</td>\n",
              "      <td>0.968544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.067275</td>\n",
              "      <td>0.774033</td>\n",
              "      <td>0.805172</td>\n",
              "      <td>0.789296</td>\n",
              "      <td>0.969177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>0.064943</td>\n",
              "      <td>0.773304</td>\n",
              "      <td>0.805747</td>\n",
              "      <td>0.789192</td>\n",
              "      <td>0.969248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.068317</td>\n",
              "      <td>0.718812</td>\n",
              "      <td>0.834483</td>\n",
              "      <td>0.772340</td>\n",
              "      <td>0.969576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.068885</td>\n",
              "      <td>0.772362</td>\n",
              "      <td>0.828736</td>\n",
              "      <td>0.799556</td>\n",
              "      <td>0.971054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.080689</td>\n",
              "      <td>0.757338</td>\n",
              "      <td>0.830460</td>\n",
              "      <td>0.792215</td>\n",
              "      <td>0.968661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.079672</td>\n",
              "      <td>0.754974</td>\n",
              "      <td>0.828736</td>\n",
              "      <td>0.790137</td>\n",
              "      <td>0.970233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>0.084221</td>\n",
              "      <td>0.763775</td>\n",
              "      <td>0.804598</td>\n",
              "      <td>0.783655</td>\n",
              "      <td>0.968403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.079203</td>\n",
              "      <td>0.738485</td>\n",
              "      <td>0.829310</td>\n",
              "      <td>0.781267</td>\n",
              "      <td>0.969271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.073154</td>\n",
              "      <td>0.740985</td>\n",
              "      <td>0.838506</td>\n",
              "      <td>0.786735</td>\n",
              "      <td>0.970655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.079185</td>\n",
              "      <td>0.762440</td>\n",
              "      <td>0.818966</td>\n",
              "      <td>0.789692</td>\n",
              "      <td>0.971218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.080009</td>\n",
              "      <td>0.756658</td>\n",
              "      <td>0.832759</td>\n",
              "      <td>0.792886</td>\n",
              "      <td>0.970139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.086785</td>\n",
              "      <td>0.764461</td>\n",
              "      <td>0.843103</td>\n",
              "      <td>0.801858</td>\n",
              "      <td>0.970069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.088488</td>\n",
              "      <td>0.771231</td>\n",
              "      <td>0.835057</td>\n",
              "      <td>0.801876</td>\n",
              "      <td>0.971077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>0.087711</td>\n",
              "      <td>0.772461</td>\n",
              "      <td>0.835057</td>\n",
              "      <td>0.802541</td>\n",
              "      <td>0.970163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.083242</td>\n",
              "      <td>0.771958</td>\n",
              "      <td>0.838506</td>\n",
              "      <td>0.803857</td>\n",
              "      <td>0.970843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.086902</td>\n",
              "      <td>0.791759</td>\n",
              "      <td>0.817241</td>\n",
              "      <td>0.804299</td>\n",
              "      <td>0.969951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.085993</td>\n",
              "      <td>0.782114</td>\n",
              "      <td>0.829310</td>\n",
              "      <td>0.805021</td>\n",
              "      <td>0.970937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.088800</td>\n",
              "      <td>0.806122</td>\n",
              "      <td>0.817241</td>\n",
              "      <td>0.811644</td>\n",
              "      <td>0.970374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.084674</td>\n",
              "      <td>0.790964</td>\n",
              "      <td>0.835057</td>\n",
              "      <td>0.812413</td>\n",
              "      <td>0.971406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.082694</td>\n",
              "      <td>0.778198</td>\n",
              "      <td>0.828736</td>\n",
              "      <td>0.802672</td>\n",
              "      <td>0.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.086283</td>\n",
              "      <td>0.806362</td>\n",
              "      <td>0.830460</td>\n",
              "      <td>0.818233</td>\n",
              "      <td>0.970374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.086967</td>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.829310</td>\n",
              "      <td>0.807725</td>\n",
              "      <td>0.970514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.088897</td>\n",
              "      <td>0.811416</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.822229</td>\n",
              "      <td>0.971265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.089342</td>\n",
              "      <td>0.812322</td>\n",
              "      <td>0.818391</td>\n",
              "      <td>0.815345</td>\n",
              "      <td>0.971453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.089816</td>\n",
              "      <td>0.794085</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.813236</td>\n",
              "      <td>0.969858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.088329</td>\n",
              "      <td>0.806761</td>\n",
              "      <td>0.822989</td>\n",
              "      <td>0.814794</td>\n",
              "      <td>0.970256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.093062</td>\n",
              "      <td>0.799124</td>\n",
              "      <td>0.839080</td>\n",
              "      <td>0.818615</td>\n",
              "      <td>0.969787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.092389</td>\n",
              "      <td>0.779707</td>\n",
              "      <td>0.825862</td>\n",
              "      <td>0.802121</td>\n",
              "      <td>0.970045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.095840</td>\n",
              "      <td>0.792079</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.809444</td>\n",
              "      <td>0.969881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.096785</td>\n",
              "      <td>0.780998</td>\n",
              "      <td>0.836207</td>\n",
              "      <td>0.807660</td>\n",
              "      <td>0.968896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.099151</td>\n",
              "      <td>0.778680</td>\n",
              "      <td>0.827011</td>\n",
              "      <td>0.802118</td>\n",
              "      <td>0.968896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.100015</td>\n",
              "      <td>0.789157</td>\n",
              "      <td>0.828161</td>\n",
              "      <td>0.808188</td>\n",
              "      <td>0.969130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.100558</td>\n",
              "      <td>0.774176</td>\n",
              "      <td>0.837356</td>\n",
              "      <td>0.804528</td>\n",
              "      <td>0.969201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.105162</td>\n",
              "      <td>0.787037</td>\n",
              "      <td>0.830460</td>\n",
              "      <td>0.808166</td>\n",
              "      <td>0.968638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.098378</td>\n",
              "      <td>0.795254</td>\n",
              "      <td>0.828161</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.969600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.100789</td>\n",
              "      <td>0.796884</td>\n",
              "      <td>0.822989</td>\n",
              "      <td>0.809726</td>\n",
              "      <td>0.969576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.103999</td>\n",
              "      <td>0.796128</td>\n",
              "      <td>0.803448</td>\n",
              "      <td>0.799771</td>\n",
              "      <td>0.968896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.104429</td>\n",
              "      <td>0.791965</td>\n",
              "      <td>0.827011</td>\n",
              "      <td>0.809109</td>\n",
              "      <td>0.969107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.105277</td>\n",
              "      <td>0.794567</td>\n",
              "      <td>0.806897</td>\n",
              "      <td>0.800684</td>\n",
              "      <td>0.969084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.105436</td>\n",
              "      <td>0.787582</td>\n",
              "      <td>0.831034</td>\n",
              "      <td>0.808725</td>\n",
              "      <td>0.969295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.107656</td>\n",
              "      <td>0.801235</td>\n",
              "      <td>0.820115</td>\n",
              "      <td>0.810565</td>\n",
              "      <td>0.969177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.112448</td>\n",
              "      <td>0.796980</td>\n",
              "      <td>0.818966</td>\n",
              "      <td>0.807823</td>\n",
              "      <td>0.968591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.109841</td>\n",
              "      <td>0.793939</td>\n",
              "      <td>0.828161</td>\n",
              "      <td>0.810689</td>\n",
              "      <td>0.968779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.106864</td>\n",
              "      <td>0.779443</td>\n",
              "      <td>0.836782</td>\n",
              "      <td>0.807095</td>\n",
              "      <td>0.969037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.107911</td>\n",
              "      <td>0.782562</td>\n",
              "      <td>0.835632</td>\n",
              "      <td>0.808227</td>\n",
              "      <td>0.969177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.107443</td>\n",
              "      <td>0.786297</td>\n",
              "      <td>0.831034</td>\n",
              "      <td>0.808047</td>\n",
              "      <td>0.969107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.106571</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.832759</td>\n",
              "      <td>0.804330</td>\n",
              "      <td>0.969060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.106385</td>\n",
              "      <td>0.768743</td>\n",
              "      <td>0.836782</td>\n",
              "      <td>0.801321</td>\n",
              "      <td>0.968896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.106788</td>\n",
              "      <td>0.770899</td>\n",
              "      <td>0.837356</td>\n",
              "      <td>0.802755</td>\n",
              "      <td>0.968966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.107309</td>\n",
              "      <td>0.771474</td>\n",
              "      <td>0.836207</td>\n",
              "      <td>0.802537</td>\n",
              "      <td>0.968943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.108337</td>\n",
              "      <td>0.779032</td>\n",
              "      <td>0.832759</td>\n",
              "      <td>0.805000</td>\n",
              "      <td>0.968943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.108412</td>\n",
              "      <td>0.779451</td>\n",
              "      <td>0.832759</td>\n",
              "      <td>0.805224</td>\n",
              "      <td>0.968990</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-477/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-954\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-954/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-954/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-954/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-954/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1431\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1431/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1431/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1431/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1431/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1908\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1908/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1908/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1908/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-1908/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2385\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2385/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2385/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2385/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2385/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2862\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2862/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2862/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2862/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-2862/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3339\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3339/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3339/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3339/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3339/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3816\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3816/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3816/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3816/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-3816/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4293\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4293/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4293/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4293/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4293/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4770\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4770/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4770/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4770/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-4770/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5247\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5247/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5247/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5247/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5247/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5724\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5724/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5724/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5724/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-5724/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6201\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6201/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6201/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6201/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6201/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6678\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6678/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6678/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6678/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-6678/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7155\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7155/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7155/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7155/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7155/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7632\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7632/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7632/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7632/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-7632/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8109\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8109/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8109/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8109/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8109/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8586\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8586/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8586/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8586/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-8586/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9063\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9063/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9063/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9063/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9063/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9540\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9540/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9540/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9540/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-9540/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10017\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10017/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10017/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10017/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10017/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10494\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10494/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10494/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10494/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10494/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10971\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10971/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10971/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10971/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-10971/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11448\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11448/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11448/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11448/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11448/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11925\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11925/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11925/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11925/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-11925/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12879\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12879/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12879/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12879/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12879/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13356\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13356/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13356/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13356/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13356/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13833\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13833/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13833/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13833/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-13833/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14310\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14310/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14310/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14310/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14310/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14787\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14787/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14787/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14787/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-14787/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15264\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15264/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15264/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15264/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15264/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15741\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15741/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15741/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15741/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-15741/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16218\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16218/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16218/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16218/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16218/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16695\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16695/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16695/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16695/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-16695/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17172\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17172/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17172/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17172/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17172/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17649\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17649/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17649/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17649/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-17649/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18126\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18126/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18126/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18126/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18126/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18603\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18603/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18603/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18603/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-18603/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19080\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19080/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19080/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19080/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19080/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19557\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19557/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19557/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19557/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-19557/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20034\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20034/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20034/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20034/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20034/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20511\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20511/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20511/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20511/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20511/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20988\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20988/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20988/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20988/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-20988/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21465\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21465/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21465/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21465/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21465/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21942\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21942/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21942/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21942/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-21942/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22419\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22419/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22419/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22419/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22419/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22896\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22896/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22896/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22896/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-22896/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23373\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23373/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23373/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23373/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23373/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23850\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23850/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23850/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23850/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-23850/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24327\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24327/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24327/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24327/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24327/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24804\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24804/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24804/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24804/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-24804/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25281\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25281/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25281/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25281/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25281/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25758\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25758/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25758/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25758/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-25758/special_tokens_map.json\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-26235\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-26235/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-26235/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-26235/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-26235/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/projeto_DL/projetoDL_BERT/output/ner-pl_corpus-bert-base-cased-pt-lenerbr/checkpoints/e55_lr2e-05/checkpoint-12402 (score: 0.8222285228239298).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=26235, training_loss=0.007261540871791574, metrics={'train_runtime': 4193.5626, 'train_samples_per_second': 62.56, 'train_steps_per_second': 6.256, 'total_flos': 7604718625743750.0, 'train_loss': 0.007261540871791574, 'epoch': 55.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Avaliação"
      ],
      "metadata": {
        "id": "o_P5XtsuqEGP"
      },
      "id": "o_P5XtsuqEGP"
    },
    {
      "cell_type": "code",
      "source": [
        "#geral\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "7WDSGK8COxnj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "7f1f2b6d-48e5-46a2-c599-079ad93bc064"
      },
      "id": "7WDSGK8COxnj",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [248/248 00:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 55.0,\n",
              " 'eval_accuracy': 0.971265041870939,\n",
              " 'eval_f1': 0.8222285228239298,\n",
              " 'eval_loss': 0.08889658749103546,\n",
              " 'eval_precision': 0.8114157806379407,\n",
              " 'eval_recall': 0.8333333333333334,\n",
              " 'eval_runtime': 8.1595,\n",
              " 'eval_samples_per_second': 303.816,\n",
              " 'eval_steps_per_second': 30.394}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predição por entidades\n",
        "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "CkDz4prVO6Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "f907c0f6-fb79-4922-a08c-dfb95b51d0eb"
      },
      "id": "CkDz4prVO6Ed",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 2479\n",
            "  Batch size = 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='496' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [248/248 00:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DATA': {'f1': 0.9047619047619048,\n",
              "  'number': 157,\n",
              "  'precision': 0.8491620111731844,\n",
              "  'recall': 0.9681528662420382},\n",
              " 'EVENTO': {'f1': 0.7692307692307693,\n",
              "  'number': 8,\n",
              "  'precision': 1.0,\n",
              "  'recall': 0.625},\n",
              " 'FUNDAMENTO': {'f1': 0.795505617977528,\n",
              "  'number': 216,\n",
              "  'precision': 0.7729257641921398,\n",
              "  'recall': 0.8194444444444444},\n",
              " 'LOCAL': {'f1': 0.8232931726907632,\n",
              "  'number': 489,\n",
              "  'precision': 0.8086785009861933,\n",
              "  'recall': 0.8384458077709611},\n",
              " 'ORGANIZACAO': {'f1': 0.8175824175824175,\n",
              "  'number': 236,\n",
              "  'precision': 0.8493150684931506,\n",
              "  'recall': 0.788135593220339},\n",
              " 'PESSOA': {'f1': 0.9136842105263158,\n",
              "  'number': 476,\n",
              "  'precision': 0.9156118143459916,\n",
              "  'recall': 0.9117647058823529},\n",
              " 'PRODUTODELEI': {'f1': 0.5180722891566265,\n",
              "  'number': 158,\n",
              "  'precision': 0.4942528735632184,\n",
              "  'recall': 0.5443037974683544},\n",
              " 'overall_accuracy': 0.971265041870939,\n",
              " 'overall_f1': 0.8222285228239298,\n",
              " 'overall_precision': 0.8114157806379407,\n",
              " 'overall_recall': 0.8333333333333334}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "collapsed_sections": [
        "o_P5XtsuqEGP"
      ],
      "name": "projetoDL_Bert.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "713bc89c7b204021a5cf5ad67e8c86ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24fc312db5dc4a5987986fc624c380d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99213bdb689a44a7889b52377c38e321",
              "IPY_MODEL_d0b41ae478044aa2bc596372cc7b34e6",
              "IPY_MODEL_088ebcdd83674bf283bff0eb2f59b9b8"
            ]
          }
        },
        "24fc312db5dc4a5987986fc624c380d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99213bdb689a44a7889b52377c38e321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59a2ea7c3f6446d98a7c6bf581055c52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d017659129a40588651a1a9725ebad4"
          }
        },
        "d0b41ae478044aa2bc596372cc7b34e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a3f4040dd834d1ab845b88b532e0809",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e672015cb3164fa289ba5db650c9424c"
          }
        },
        "088ebcdd83674bf283bff0eb2f59b9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18b2013cca3c4705b124ed5e3a44504d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 47.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d51bb0f4bc7481c9f5ff5f5a3ebdc25"
          }
        },
        "59a2ea7c3f6446d98a7c6bf581055c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d017659129a40588651a1a9725ebad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a3f4040dd834d1ab845b88b532e0809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e672015cb3164fa289ba5db650c9424c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18b2013cca3c4705b124ed5e3a44504d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d51bb0f4bc7481c9f5ff5f5a3ebdc25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}